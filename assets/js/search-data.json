{
  
    
        "post0": {
            "title": "Geospatial analysis",
            "content": ". Setup . Your first map . Roadtrip planning . import geopandas as gpd # Read in the data full_data = gpd.read_file(&quot;/content/geospatial-learn-course-data/DEC_lands/DEC_lands/DEC_lands.shp&quot;) # View the first five rows of the data full_data.head() . OBJECTID CATEGORY UNIT FACILITY CLASS UMP DESCRIPTIO REGION COUNTY URL SOURCE UPDATE_ OFFICE ACRES LANDS_UID GREENCERT SHAPE_AREA SHAPE_LEN geometry . 0 1 | FOR PRES DET PAR | CFP | HANCOCK FP DETACHED PARCEL | WILD FOREST | None | DELAWARE COUNTY DETACHED PARCEL | 4 | DELAWARE | http://www.dec.ny.gov/ | DELAWARE RPP | 5/12 | STAMFORD | 738.620192 | 103 | N | 2.990365e+06 | 7927.662385 | POLYGON ((486093.245 4635308.586, 486787.235 4... | . 1 2 | FOR PRES DET PAR | CFP | HANCOCK FP DETACHED PARCEL | WILD FOREST | None | DELAWARE COUNTY DETACHED PARCEL | 4 | DELAWARE | http://www.dec.ny.gov/ | DELAWARE RPP | 5/12 | STAMFORD | 282.553140 | 1218 | N | 1.143940e+06 | 4776.375600 | POLYGON ((491931.514 4637416.256, 491305.424 4... | . 2 3 | FOR PRES DET PAR | CFP | HANCOCK FP DETACHED PARCEL | WILD FOREST | None | DELAWARE COUNTY DETACHED PARCEL | 4 | DELAWARE | http://www.dec.ny.gov/ | DELAWARE RPP | 5/12 | STAMFORD | 234.291262 | 1780 | N | 9.485476e+05 | 5783.070364 | POLYGON ((486000.287 4635834.453, 485007.550 4... | . 3 4 | FOR PRES DET PAR | CFP | GREENE COUNTY FP DETACHED PARCEL | WILD FOREST | None | None | 4 | GREENE | http://www.dec.ny.gov/ | GREENE RPP | 5/12 | STAMFORD | 450.106464 | 2060 | N | 1.822293e+06 | 7021.644833 | POLYGON ((541716.775 4675243.268, 541217.579 4... | . 4 6 | FOREST PRESERVE | AFP | SARANAC LAKES WILD FOREST | WILD FOREST | SARANAC LAKES | None | 5 | ESSEX | http://www.dec.ny.gov/lands/22593.html | DECRP, ESSEX RPP | 12/96 | RAY BROOK | 69.702387 | 1517 | N | 2.821959e+05 | 2663.909932 | POLYGON ((583896.043 4909643.187, 583891.200 4... | . data = full_data.loc[:, [&quot;CLASS&quot;, &quot;COUNTY&quot;, &quot;geometry&quot;]].copy() # How many lands of each type are there? data.CLASS.value_counts() . WILD FOREST 965 INTENSIVE USE 108 PRIMITIVE 60 WILDERNESS 52 ADMINISTRATIVE 17 UNCLASSIFIED 7 HISTORIC 5 PRIMITIVE BICYCLE CORRIDOR 4 CANOE AREA 1 Name: CLASS, dtype: int64 . wild_lands = data.loc[data.CLASS.isin([&#39;WILD FOREST&#39;, &#39;WILDERNESS&#39;])].copy() wild_lands.head() . CLASS COUNTY geometry . 0 WILD FOREST | DELAWARE | POLYGON ((486093.245 4635308.586, 486787.235 4... | . 1 WILD FOREST | DELAWARE | POLYGON ((491931.514 4637416.256, 491305.424 4... | . 2 WILD FOREST | DELAWARE | POLYGON ((486000.287 4635834.453, 485007.550 4... | . 3 WILD FOREST | GREENE | POLYGON ((541716.775 4675243.268, 541217.579 4... | . 4 WILD FOREST | ESSEX | POLYGON ((583896.043 4909643.187, 583891.200 4... | . wild_lands.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5d499cc150&gt; . wild_lands.geometry.head() . 0 POLYGON ((486093.245 4635308.586, 486787.235 4... 1 POLYGON ((491931.514 4637416.256, 491305.424 4... 2 POLYGON ((486000.287 4635834.453, 485007.550 4... 3 POLYGON ((541716.775 4675243.268, 541217.579 4... 4 POLYGON ((583896.043 4909643.187, 583891.200 4... Name: geometry, dtype: geometry . POI_data = gpd.read_file(&quot;/content/geospatial-learn-course-data/DEC_pointsinterest/DEC_pointsinterest/Decptsofinterest.shp&quot;) campsites = POI_data.loc[POI_data.ASSET==&#39;PRIMITIVE CAMPSITE&#39;].copy() # Foot trails in New York state (LineString) roads_trails = gpd.read_file(&quot;/content/geospatial-learn-course-data/DEC_roadstrails/DEC_roadstrails/Decroadstrails.shp&quot;) trails = roads_trails.loc[roads_trails.ASSET==&#39;FOOT TRAIL&#39;].copy() # County boundaries in New York state (Polygon) counties = gpd.read_file(&quot;/content/geospatial-learn-course-data/NY_county_boundaries/NY_county_boundaries/NY_county_boundaries.shp&quot;) . ax = counties.plot(figsize=(50,50), color=&#39;none&#39;, edgecolor=&#39;gainsboro&#39;, zorder=3) # Add wild lands, campsites, and foot trails to the base map wild_lands.plot(color=&#39;lightgreen&#39;, ax=ax) campsites.plot(color=&#39;maroon&#39;, markersize=2, ax=ax) trails.plot(color=&#39;black&#39;, markersize=1, ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5d4959b410&gt; . Kiwa loans . kiwa_loans = gpd.read_file(&quot;/content/geospatial-learn-course-data/kiva_loans/kiva_loans/kiva_loans.shp&quot;) kiwa_loans.head() . Partner ID Field Part sector Loan Theme country amount geometry . 0 9 | KREDIT Microfinance Institution | General Financial Inclusion | Higher Education | Cambodia | 450 | POINT (102.89751 13.66726) | . 1 9 | KREDIT Microfinance Institution | General Financial Inclusion | Vulnerable Populations | Cambodia | 20275 | POINT (102.98962 13.02870) | . 2 9 | KREDIT Microfinance Institution | General Financial Inclusion | Higher Education | Cambodia | 9150 | POINT (102.98962 13.02870) | . 3 9 | KREDIT Microfinance Institution | General Financial Inclusion | Vulnerable Populations | Cambodia | 604950 | POINT (105.31312 12.09829) | . 4 9 | KREDIT Microfinance Institution | General Financial Inclusion | Sanitation | Cambodia | 275 | POINT (105.31312 12.09829) | . world_filepath = gpd.datasets.get_path(&#39;naturalearth_lowres&#39;) world = gpd.read_file(world_filepath) world.head() . pop_est continent name iso_a3 gdp_md_est geometry . 0 920938 | Oceania | Fiji | FJI | 8374.0 | MULTIPOLYGON (((180.00000 -16.06713, 180.00000... | . 1 53950935 | Africa | Tanzania | TZA | 150600.0 | POLYGON ((33.90371 -0.95000, 34.07262 -1.05982... | . 2 603253 | Africa | W. Sahara | ESH | 906.5 | POLYGON ((-8.66559 27.65643, -8.66512 27.58948... | . 3 35623680 | North America | Canada | CAN | 1674000.0 | MULTIPOLYGON (((-122.84000 49.00000, -122.9742... | . 4 326625791 | North America | United States of America | USA | 18560000.0 | MULTIPOLYGON (((-122.84000 49.00000, -120.0000... | . ax = world.plot(figsize=(50,50), color=&#39;none&#39;, edgecolor=&#39;grey&#39;, zorder=3) # Add wild lands, campsites, and foot trails to the base map kiwa_loans.plot(color=&#39;lightgreen&#39;, ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5d4957a450&gt; . PHL_loans = kiwa_loans.loc[kiwa_loans.country == &#39;Philippines&#39;].copy() PHL_loans.head() . Partner ID Field Part sector Loan Theme country amount geometry . 2859 123 | Alalay sa Kaunlaran (ASKI) | General Financial Inclusion | General | Philippines | 400 | POINT (121.73961 17.64228) | . 2860 123 | Alalay sa Kaunlaran (ASKI) | General Financial Inclusion | General | Philippines | 400 | POINT (121.74169 17.63235) | . 2861 123 | Alalay sa Kaunlaran (ASKI) | General Financial Inclusion | General | Philippines | 400 | POINT (121.46667 16.60000) | . 2862 123 | Alalay sa Kaunlaran (ASKI) | General Financial Inclusion | General | Philippines | 6050 | POINT (121.73333 17.83333) | . 2863 123 | Alalay sa Kaunlaran (ASKI) | General Financial Inclusion | General | Philippines | 625 | POINT (121.51800 16.72368) | . gpd.io.file.fiona.drvsupport.supported_drivers[&#39;KML&#39;] = &#39;rw&#39; PHL = gpd.read_file(&quot;/content/geospatial-learn-course-data/Philippines_AL258.kml&quot;, driver=&#39;KML&#39;) PHL.head() . /usr/local/lib/python3.7/dist-packages/geopandas/geodataframe.py:577: RuntimeWarning: Sequential read of iterator was interrupted. Resetting iterator. This can negatively impact the performance. for feature in features_lst: . Name Description geometry . 0 Autonomous Region in Muslim Mindanao | | MULTIPOLYGON (((119.46690 4.58718, 119.46653 4... | . 1 Bicol Region | | MULTIPOLYGON (((124.04577 11.57862, 124.04594 ... | . 2 Cagayan Valley | | MULTIPOLYGON (((122.51581 17.04436, 122.51568 ... | . 3 Calabarzon | | MULTIPOLYGON (((120.49202 14.05403, 120.49201 ... | . 4 Caraga | | MULTIPOLYGON (((126.45401 8.24400, 126.45407 8... | . ax = PHL.plot(figsize=(20,20), color=&#39;none&#39;, edgecolor=&#39;grey&#39;, zorder=3) # Add wild lands, campsites, and foot trails to the base map PHL_loans.plot(color=&#39;lightgreen&#39;, ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5d4014e6d0&gt; . Coordinate Reference Systems . Ghana . regions = gpd.read_file(&quot;/content/geospatial-learn-course-data/ghana/ghana/Regions/Map_of_Regions_in_Ghana.shp&quot;) print(regions.crs) . epsg:32630 . import pandas as pd # Create a DataFrame with health facilities in Ghana facilities_df = pd.read_csv(&quot;/content/geospatial-learn-course-data/ghana/ghana/health_facilities.csv&quot;) # Convert the DataFrame to a GeoDataFrame facilities = gpd.GeoDataFrame(facilities_df, geometry=gpd.points_from_xy(facilities_df.Longitude, facilities_df.Latitude)) # Set the coordinate reference system (CRS) to EPSG 4326 facilities.crs = {&#39;init&#39;: &#39;epsg:4326&#39;} # View the first five rows of the GeoDataFrame facilities.head() . /usr/local/lib/python3.7/dist-packages/pyproj/crs/crs.py:53: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 return _prepare_from_string(&#34; &#34;.join(pjargs)) . Region District FacilityName Type Town Ownership Latitude Longitude geometry . 0 Ashanti | Offinso North | A.M.E Zion Clinic | Clinic | Afrancho | CHAG | 7.40801 | -1.96317 | POINT (-1.96317 7.40801) | . 1 Ashanti | Bekwai Municipal | Abenkyiman Clinic | Clinic | Anwiankwanta | Private | 6.46312 | -1.58592 | POINT (-1.58592 6.46312) | . 2 Ashanti | Adansi North | Aboabo Health Centre | Health Centre | Aboabo No 2 | Government | 6.22393 | -1.34982 | POINT (-1.34982 6.22393) | . 3 Ashanti | Afigya-Kwabre | Aboabogya Health Centre | Health Centre | Aboabogya | Government | 6.84177 | -1.61098 | POINT (-1.61098 6.84177) | . 4 Ashanti | Kwabre | Aboaso Health Centre | Health Centre | Aboaso | Government | 6.84177 | -1.61098 | POINT (-1.61098 6.84177) | . ax = regions.plot(figsize=(8,8), color=&#39;whitesmoke&#39;, linestyle=&#39;:&#39;, edgecolor=&#39;black&#39;) facilities.to_crs(epsg=32630).plot(markersize=1, ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5d3fe7ac10&gt; . regions.to_crs(&quot;+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs&quot;).head() . Region geometry . 0 Ashanti | POLYGON ((-1.30985 7.62302, -1.30786 7.62198, ... | . 1 Brong Ahafo | POLYGON ((-2.54567 8.76089, -2.54473 8.76071, ... | . 2 Central | POLYGON ((-2.06723 6.29473, -2.06658 6.29420, ... | . 3 Eastern | POLYGON ((-0.21751 7.21009, -0.21747 7.20993, ... | . 4 Greater Accra | POLYGON ((0.23456 6.10986, 0.23484 6.10974, 0.... | . facilities.geometry.x.head() . 0 -1.96317 1 -1.58592 2 -1.34982 3 -1.61098 4 -1.61098 dtype: float64 . regions.loc[:, &quot;AREA&quot;] = regions.geometry.area / 10**6 print(&quot;Area of Ghana: {} square kilometers&quot;.format(regions.AREA.sum())) print(&quot;CRS:&quot;, regions.crs) regions.head() . Area of Ghana: 239584.5760055668 square kilometers CRS: epsg:32630 . Region geometry AREA . 0 Ashanti | POLYGON ((686446.075 842986.894, 686666.193 84... | 24379.017777 | . 1 Brong Ahafo | POLYGON ((549970.457 968447.094, 550073.003 96... | 40098.168231 | . 2 Central | POLYGON ((603176.584 695877.238, 603248.424 69... | 9665.626760 | . 3 Eastern | POLYGON ((807307.254 797910.553, 807311.908 79... | 18987.625847 | . 4 Greater Accra | POLYGON ((858081.638 676424.913, 858113.115 67... | 3706.511145 | . Bird conservation . birds_df = pd.read_csv(&quot;/content/geospatial-learn-course-data/purple_martin.csv&quot;, parse_dates=[&#39;timestamp&#39;]) print(&quot;There are {} different birds in the dataset.&quot;.format(birds_df[&quot;tag-local-identifier&quot;].nunique())) birds_df.head() . There are 11 different birds in the dataset. . timestamp location-long location-lat tag-local-identifier . 0 2014-08-15 05:56:00 | -88.146014 | 17.513049 | 30448 | . 1 2014-09-01 05:59:00 | -85.243501 | 13.095782 | 30448 | . 2 2014-10-30 23:58:00 | -62.906089 | -7.852436 | 30448 | . 3 2014-11-15 04:59:00 | -61.776826 | -11.723898 | 30448 | . 4 2014-11-30 09:59:00 | -61.241538 | -11.612237 | 30448 | . birds_df[&#39;location-long&#39;] . 0 -88.146014 1 -85.243501 2 -62.906089 3 -61.776826 4 -61.241538 ... 94 -50.709645 95 -49.292113 96 -49.081317 97 -49.081245 98 -50.192297 Name: location-long, Length: 99, dtype: float64 . birds = gpd.GeoDataFrame(birds_df, geometry=gpd.points_from_xy(birds_df[&#39;location-long&#39;], birds_df[&#39;location-lat&#39;])) birds.crs = {&#39;init&#39;: &#39;epsg:4326&#39;} birds.head() . /usr/local/lib/python3.7/dist-packages/pyproj/crs/crs.py:53: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 return _prepare_from_string(&#34; &#34;.join(pjargs)) . timestamp location-long location-lat tag-local-identifier geometry . 0 2014-08-15 05:56:00 | -88.146014 | 17.513049 | 30448 | POINT (-88.14601 17.51305) | . 1 2014-09-01 05:59:00 | -85.243501 | 13.095782 | 30448 | POINT (-85.24350 13.09578) | . 2 2014-10-30 23:58:00 | -62.906089 | -7.852436 | 30448 | POINT (-62.90609 -7.85244) | . 3 2014-11-15 04:59:00 | -61.776826 | -11.723898 | 30448 | POINT (-61.77683 -11.72390) | . 4 2014-11-30 09:59:00 | -61.241538 | -11.612237 | 30448 | POINT (-61.24154 -11.61224) | . world = gpd.read_file(gpd.datasets.get_path(&#39;naturalearth_lowres&#39;)) americas = world.loc[world[&#39;continent&#39;].isin([&#39;North America&#39;, &#39;South America&#39;])] americas.head() . pop_est continent name iso_a3 gdp_md_est geometry . 3 35623680 | North America | Canada | CAN | 1674000.0 | MULTIPOLYGON (((-122.84000 49.00000, -122.9742... | . 4 326625791 | North America | United States of America | USA | 18560000.0 | MULTIPOLYGON (((-122.84000 49.00000, -120.0000... | . 9 44293293 | South America | Argentina | ARG | 879400.0 | MULTIPOLYGON (((-68.63401 -52.63637, -68.25000... | . 10 17789267 | South America | Chile | CHL | 436100.0 | MULTIPOLYGON (((-68.63401 -52.63637, -68.63335... | . 16 10646714 | North America | Haiti | HTI | 19340.0 | POLYGON ((-71.71236 19.71446, -71.62487 19.169... | . ax = americas.plot(figsize=(8,8), color=&#39;whitesmoke&#39;, linestyle=&#39;:&#39;, edgecolor=&#39;black&#39;) birds.plot(markersize=1, ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5d3f9325d0&gt; . from shapely.geometry import LineString # GeoDataFrame showing path for each bird path_df = birds.groupby(&quot;tag-local-identifier&quot;)[&#39;geometry&#39;].apply(list).apply(lambda x: LineString(x)).reset_index() path_gdf = gpd.GeoDataFrame(path_df, geometry=path_df.geometry) path_gdf.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} # GeoDataFrame showing starting point for each bird start_df = birds.groupby(&quot;tag-local-identifier&quot;)[&#39;geometry&#39;].apply(list).apply(lambda x: x[0]).reset_index() start_gdf = gpd.GeoDataFrame(start_df, geometry=start_df.geometry) start_gdf.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} # GeoDataFrame showing ending point for each bird end_df = birds.groupby(&quot;tag-local-identifier&quot;)[&#39;geometry&#39;].apply(list).apply(lambda x: x[-1]).reset_index() end_gdf = gpd.GeoDataFrame(end_df, geometry=end_df.geometry) end_gdf.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} . /usr/local/lib/python3.7/dist-packages/pyproj/crs/crs.py:53: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 return _prepare_from_string(&#34; &#34;.join(pjargs)) /usr/local/lib/python3.7/dist-packages/pyproj/crs/crs.py:53: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 return _prepare_from_string(&#34; &#34;.join(pjargs)) /usr/local/lib/python3.7/dist-packages/pyproj/crs/crs.py:53: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 return _prepare_from_string(&#34; &#34;.join(pjargs)) . ax = americas.plot(figsize=(8,8), color=&#39;whitesmoke&#39;, linestyle=&#39;:&#39;, edgecolor=&#39;black&#39;) #start_gdf.plot(markersize=1, ax=ax) end_gdf.plot(markersize=1, ax=ax) #path_gdf.plot(markersize=1, ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5d3fa52710&gt; . protected_areas = gpd.read_file(&quot;/content/geospatial-learn-course-data/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile-polygons.shp&quot;) . south_america = americas.loc[americas[&#39;continent&#39;]==&#39;South America&#39;] # Create a map ax = south_america.plot(figsize=(8,8), color=&#39;whitesmoke&#39;, linestyle=&#39;:&#39;, edgecolor=&#39;black&#39;) protected_areas.plot(markersize=1, ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5d3fb1fa10&gt; . P_Area = sum(protected_areas[&#39;REP_AREA&#39;]-protected_areas[&#39;REP_M_AREA&#39;]) print(&quot;South America has {} square kilometers of protected areas.&quot;.format(P_Area)) . South America has 5396761.9116883585 square kilometers of protected areas. . south_america.head() . pop_est continent name iso_a3 gdp_md_est geometry . 9 44293293 | South America | Argentina | ARG | 879400.0 | MULTIPOLYGON (((-68.63401 -52.63637, -68.25000... | . 10 17789267 | South America | Chile | CHL | 436100.0 | MULTIPOLYGON (((-68.63401 -52.63637, -68.63335... | . 20 2931 | South America | Falkland Is. | FLK | 281.8 | POLYGON ((-61.20000 -51.85000, -60.00000 -51.2... | . 28 3360148 | South America | Uruguay | URY | 73250.0 | POLYGON ((-57.62513 -30.21629, -56.97603 -30.1... | . 29 207353391 | South America | Brazil | BRA | 3081000.0 | POLYGON ((-53.37366 -33.76838, -53.65054 -33.2... | . Finish part 7 and 8!!! .",
            "url": "https://lenguist.github.io/site/2021/03/14/geospatial.html",
            "relUrl": "/2021/03/14/geospatial.html",
            "date": " • Mar 14, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "What is AI __really__?",
            "content": "What is AI really? . Intro . So I was doing computer vision tutorial on kaggle and it had been really interesting to learn imaage processing AFTER NLP and not vice versa. It’s remarkable hwo similar the development of those fields are, and I had some interesting thoughts on it. Enjoy! . AI vs DL vs ML vs Data Science . From my experience/knowledge/uderstanding, AI is a very specific term which is used somewhat interchangeable with ML and DL by many people. It is interesting how when learning more stuff about ML/DL/AI this distinction became more clear and intuitive for me. Data science is the field of data analysis in general, so pretty much anything you do involving data is data science. The term is so broad it is somewhat useless in my opinion. Then we have ML. ML is about statistics and “learning” data representations. I do not remember where I heard it, but someone said ML is statictics plus, and that is kind of how I like to think about it. It is a very math-heavy field, and, imo, pretty boring one. Saying that, I also did not really do much ML, so that is a pretty uneducated opinion. . Then we have DL. Deep learning, by definition, is just a type of machine learning which uses “deep nets.” And I thinnk that is … a good definition. It leads, ultimately, to the less obvious differences between ML annd DL. Because we have many hidden layers, DL necesarily acts like a black box. Also, DL is much less statistics and much more architecture and feature engineereing. Here you have CNNs, and RNNs, and Transformers, and GANs, and those “architectures” are much more conceptually based then ML algorithms. Like, youu get an idea for what RNN is and why you need and when you need because the structure itself is a result of human intuition rather than careful proof. There is nno proof that RNNs would be good for sequential data; there is probably not even a way to rigurously define “good” in this context. It is not like some mathematician somewhere proved that yeah, if you have sequential data, rnns would be great. It’s an intuitive idea, which turned out to be true, and that’s really fascinating. In DL,architectures are modeled by the way we, humans, think about stuff. Embeddings are not something “the statistics” would suggest as a solution; it is purely taken from the way humans think about words and complex categorization. For me, this what makes DL a much more compelling field then ML: the fact that it is “closer” to the mind, and closer to AI. . And what is AI? Is image recognition an AI? Is voice detection an AI? Is alpha Zero a AI? Is Dall-e an AI? Is GPT an AI? For me, some of those examples feel more like AI then others, but it is hard to pinpoint what makes them “more AI.” I think it might be the degree to which they can innterract with humans? That is a bad definion though; it has nothing to do with the AI itself, and more with how we use it. Still though, image recognition system is predictable: it will recognize images. We kind of, in a way, have a clea expectation of what it is supposed to do. Same with voice detection. Alpha zero is … trickier. It’s suposed to be good at chess, but we dont have such specific expectations for its actions. We dont wait for it to make a particular move. In a way, it feels “more-AI” because it surpasses humans in its activity, making us unable to set concrete expectations. I think the same goes for Dall-e and GPT: they are so good, we do not know what to expect to of them. But there is more to it: they are “generative.” I put quotation marks around that beccause in DL the term generative model has a specific meaning and technically all five would be “generating” something. By generating I more likely mean “repurposing” human input. Image recognition does not create new information for us, humans. Same with voie detection. Alpha Zero makes a move based on human input, not only “translating it.” Same with Dall-e or GPT - they generate new information on human input, and not just convert the information from one form to another. . I got off track. So, for me, AI seems like too broad of a definiton really. I think AI must be neccesarily an agent which a human has to be able to interact with. Image recognition is not AI in my opinion: it is a DL model with a very specific, predefined purpose. What makes AI “more-AI” is it’s capability to create unique information. . Wow. That was a nice revelation. So yeah, I also want to talk about use cases of AI vs DL, but for now, let’s talk more about this “more-AI” concept because it is really fascinating. What makes AGI? ability to create unique information without human input. In other words, it has to be self-sufficient. I mean, this is so huge. It has to be adaptive, and it has to self-sufficient. . So … let’s talk about those conditions. . Adaptive AI . I think this is gonna be a major research topic in the coming years. Because now, with the advent of “lucky ticket” concept for neural nets, the magic kinda wastes away and you see them for what they are - limited, rigid, functions, which we have a convoluted way to findd and define. But, inteligence is more than that. Ultimately, it’s not even the fact that human intelligence is different from DL models, it’s the fact those models fail in important ways because they lack adaptability. Again, as I am thinking about a possible solution here, I am coming back to human brainn and how it works, which is kinda interesting. You dont ddo that much in ML; there, you turn to math. I mean maybe you should turn to math here as well. What is a way to make something adaptable? What do we mean by adaptable? How much data should the model need to adapt? We dont want it freaking out with every example, but we also dont want to wait years to have enough data for a retrain. And also the process of training should be .. constant? how woudl that work? Maybe doing some reflexive module, where the model trains trying to understand itself? Or weight new examples higher than old to make them more relevant? Or just collect mroe data, so that the continuous training concept makes sense. Our minds are laways in motion; we never stop thinking. The model does; it never thinks. It’s a function, a state of AI, so to say. what we call AI is really a snapshot of an AI. . Which brings us to reinforcement learning? to make AI adaptive, we want it to learn from experiences. That sentence made 0 sense. I feel like RL is the solution, my intuition tells me that, but I can not quite put into words why I think that. . Perceptive AI . I will add this category in, because that’s what I wanted to talk about initially, but I will go over it briefly. Humans collect a lot of data to worm with constantly. We need AI doing the same. Text + image + voice, but all in one model. . To be continued… . My brain hurts. I need to turn this into a series, add pictures, and be mroe concise with my writing. Will do tmrw. .",
            "url": "https://lenguist.github.io/site/thoughts/2021/03/13/perceptive-ai.html",
            "relUrl": "/thoughts/2021/03/13/perceptive-ai.html",
            "date": " • Mar 13, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Computer vision",
            "content": ". Setup . #Need to upload the kaggle.json file first !mkdir ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json !pip install --upgrade --force-reinstall --no-deps kaggle . mkdir: cannot create directory ‘/root/.kaggle’: File exists Processing /root/.cache/pip/wheels/a1/6a/26/d30b7499ff85a4a4593377a87ecf55f7d08af42f0de9b60303/kaggle-1.5.12-cp37-none-any.whl Installing collected packages: kaggle Found existing installation: kaggle 1.5.12 Uninstalling kaggle-1.5.12: Successfully uninstalled kaggle-1.5.12 Successfully installed kaggle-1.5.12 . The Convolutional Classifier . Getting data . !kaggle datasets download -d ryanholbrook/car-or-truck . Downloading car-or-truck.zip to /content 67% 52.0M/77.7M [00:01&lt;00:00, 32.0MB/s] 100% 77.7M/77.7M [00:01&lt;00:00, 71.5MB/s] . Code . pretrained_base = tf.keras.applications.VGG16(include_top=False, input_shape=(128, 128, 3)) pretrained_base.trainable = False . from tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ pretrained_base, layers.Flatten(), layers.Dense(6, activation=&#39;relu&#39;), layers.Dense(1, activation=&#39;sigmoid&#39;), ]) . model.compile( optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;binary_accuracy&#39;], ) history = model.fit( ds_train, validation_data=ds_valid, epochs=30, ) . import pandas as pd history_frame = pd.DataFrame(history.history) history_frame.loc[:, [&#39;loss&#39;, &#39;val_loss&#39;]].plot() history_frame.loc[:, [&#39;binary_accuracy&#39;, &#39;val_binary_accuracy&#39;]].plot(); . model.save(&#39;/content/drive/MyDrive/Code/model_directories&#39;) . Convolution and ReLU . import numpy as np from itertools import product def show_kernel(kernel, label=True, digits=None, text_size=28): # Format kernel kernel = np.array(kernel) if digits is not None: kernel = kernel.round(digits) # Plot kernel cmap = plt.get_cmap(&#39;Blues_r&#39;) plt.imshow(kernel, cmap=cmap) rows, cols = kernel.shape thresh = (kernel.max()+kernel.min())/2 # Optionally, add value labels if label: for i, j in product(range(rows), range(cols)): val = kernel[i, j] color = cmap(0) if val &gt; thresh else cmap(255) plt.text(j, i, val, color=color, size=text_size, horizontalalignment=&#39;center&#39;, verticalalignment=&#39;center&#39;) plt.xticks([]) plt.yticks([]) . from tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ layers.Conv2D(filters=64, kernel_size=3, activation=&#39;relu&#39;), # activation is None # More layers follow ]) . !kaggle datasets download -d ryanholbrook/computer-vision-resources !unzip computer-vision-resources.zip -d computer-vision-resources . Downloading computer-vision-resources.zip to /content 38% 5.00M/13.1M [00:00&lt;00:00, 30.7MB/s] 100% 13.1M/13.1M [00:00&lt;00:00, 62.8MB/s] Archive: computer-vision-resources.zip inflating: computer-vision-resources/car_1.jpg inflating: computer-vision-resources/car_feature.jpg inflating: computer-vision-resources/car_illus.jpg inflating: computer-vision-resources/circle_feature_center.png inflating: computer-vision-resources/circle_feature_ul.png inflating: computer-vision-resources/goose.png inflating: computer-vision-resources/jw.jpg inflating: computer-vision-resources/k.jpg inflating: computer-vision-resources/k.png inflating: computer-vision-resources/kernel_down.jpg inflating: computer-vision-resources/kerneler.png inflating: computer-vision-resources/ma.jpg inflating: computer-vision-resources/machinelearning.csv inflating: computer-vision-resources/maxpool.png inflating: computer-vision-resources/maxpool2.png inflating: computer-vision-resources/rl.jpg inflating: computer-vision-resources/tensorflow_datasets-3.2.1-py3-none-any.whl inflating: computer-vision-resources/ys.jpg . import tensorflow as tf import matplotlib.pyplot as plt plt.rc(&#39;figure&#39;, autolayout=True) plt.rc(&#39;axes&#39;, labelweight=&#39;bold&#39;, labelsize=&#39;large&#39;, titleweight=&#39;bold&#39;, titlesize=18, titlepad=10) plt.rc(&#39;image&#39;, cmap=&#39;magma&#39;) image_path = &#39;/content/computer-vision-resources/car_feature.jpg&#39; image = tf.io.read_file(image_path) image = tf.io.decode_jpeg(image) plt.figure(figsize=(6, 6)) plt.imshow(tf.squeeze(image), cmap=&#39;gray&#39;) plt.axis(&#39;off&#39;) plt.show(); . import tensorflow as tf kernel = tf.constant([ [-1, -1, -1], [-1, 8, -1], [-1, -1, -1], ]) plt.figure(figsize=(3, 3)) show_kernel(kernel) . image = tf.image.convert_image_dtype(image, dtype=tf.float32) image = tf.expand_dims(image, axis=0) kernel = tf.reshape(kernel, [*kernel.shape, 1, 1]) kernel = tf.cast(kernel, dtype=tf.float32) . image_filter = tf.nn.conv2d( input=image, filters=kernel, # we&#39;ll talk about these two in lesson 4! strides=1, padding=&#39;SAME&#39;, ) plt.figure(figsize=(6, 6)) plt.imshow(tf.squeeze(image_filter)) plt.axis(&#39;off&#39;) plt.show(); . image_detect = tf.nn.relu(image_filter) plt.figure(figsize=(6, 6)) plt.imshow(tf.squeeze(image_detect)) plt.axis(&#39;off&#39;) plt.show(); . Maximum pooling . from tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ layers.Conv2D(filters=64, kernel_size=3), # activation is None layers.MaxPool2D(pool_size=2), # More layers follow ]) . import tensorflow as tf import matplotlib.pyplot as plt import warnings plt.rc(&#39;figure&#39;, autolayout=True) plt.rc(&#39;axes&#39;, labelweight=&#39;bold&#39;, labelsize=&#39;large&#39;, titleweight=&#39;bold&#39;, titlesize=18, titlepad=10) plt.rc(&#39;image&#39;, cmap=&#39;magma&#39;) warnings.filterwarnings(&quot;ignore&quot;) # to clean up output cells # Read image image_path = &#39;/content/computer-vision-resources/car_feature.jpg&#39; image = tf.io.read_file(image_path) image = tf.io.decode_jpeg(image) # Define kernel kernel = tf.constant([ [-1, -1, -1], [-1, 8, -1], [-1, -1, -1], ], dtype=tf.float32) # Reformat for batch compatibility. image = tf.image.convert_image_dtype(image, dtype=tf.float32) image = tf.expand_dims(image, axis=0) kernel = tf.reshape(kernel, [*kernel.shape, 1, 1]) # Filter step image_filter = tf.nn.conv2d( input=image, filters=kernel, # we&#39;ll talk about these two in the next lesson! strides=1, padding=&#39;SAME&#39; ) # Detect step image_detect = tf.nn.relu(image_filter) # Show what we have so far plt.figure(figsize=(12, 6)) plt.subplot(131) plt.imshow(tf.squeeze(image), cmap=&#39;gray&#39;) plt.axis(&#39;off&#39;) plt.title(&#39;Input&#39;) plt.subplot(132) plt.imshow(tf.squeeze(image_filter)) plt.axis(&#39;off&#39;) plt.title(&#39;Filter&#39;) plt.subplot(133) plt.imshow(tf.squeeze(image_detect)) plt.axis(&#39;off&#39;) plt.title(&#39;Detect&#39;) plt.show(); . import tensorflow as tf image_condense = tf.nn.pool( input=image_detect, # image in the Detect step above window_shape=(2, 2), pooling_type=&#39;MAX&#39;, # we&#39;ll see what these do in the next lesson! strides=(2, 2), padding=&#39;SAME&#39;, ) plt.figure(figsize=(6, 6)) plt.imshow(tf.squeeze(image_condense)) plt.axis(&#39;off&#39;) plt.show(); . The Sliding Window . import numpy as np from itertools import product from skimage import draw, transform def circle(size, val=None, r_shrink=0): circle = np.zeros([size[0]+1, size[1]+1]) rr, cc = draw.circle_perimeter( size[0]//2, size[1]//2, radius=size[0]//2 - r_shrink, shape=[size[0]+1, size[1]+1], ) if val is None: circle[rr, cc] = np.random.uniform(size=circle.shape)[rr, cc] else: circle[rr, cc] = val circle = transform.resize(circle, size, order=0) return circle def show_kernel(kernel, label=True, digits=None, text_size=28): # Format kernel kernel = np.array(kernel) if digits is not None: kernel = kernel.round(digits) # Plot kernel cmap = plt.get_cmap(&#39;Blues_r&#39;) plt.imshow(kernel, cmap=cmap) rows, cols = kernel.shape thresh = (kernel.max()+kernel.min())/2 # Optionally, add value labels if label: for i, j in product(range(rows), range(cols)): val = kernel[i, j] color = cmap(0) if val &gt; thresh else cmap(255) plt.text(j, i, val, color=color, size=text_size, horizontalalignment=&#39;center&#39;, verticalalignment=&#39;center&#39;) plt.xticks([]) plt.yticks([]) def show_extraction(image, kernel, conv_stride=1, conv_padding=&#39;valid&#39;, activation=&#39;relu&#39;, pool_size=2, pool_stride=2, pool_padding=&#39;same&#39;, figsize=(10, 10), subplot_shape=(2, 2), ops=[&#39;Input&#39;, &#39;Filter&#39;, &#39;Detect&#39;, &#39;Condense&#39;], gamma=1.0): # Create Layers model = tf.keras.Sequential([ tf.keras.layers.Conv2D( filters=1, kernel_size=kernel.shape, strides=conv_stride, padding=conv_padding, use_bias=False, input_shape=image.shape, ), tf.keras.layers.Activation(activation), tf.keras.layers.MaxPool2D( pool_size=pool_size, strides=pool_stride, padding=pool_padding, ), ]) layer_filter, layer_detect, layer_condense = model.layers kernel = tf.reshape(kernel, [*kernel.shape, 1, 1]) layer_filter.set_weights([kernel]) # Format for TF image = tf.expand_dims(image, axis=0) image = tf.image.convert_image_dtype(image, dtype=tf.float32) # Extract Feature image_filter = layer_filter(image) image_detect = layer_detect(image_filter) image_condense = layer_condense(image_detect) images = {} if &#39;Input&#39; in ops: images.update({&#39;Input&#39;: (image, 1.0)}) if &#39;Filter&#39; in ops: images.update({&#39;Filter&#39;: (image_filter, 1.0)}) if &#39;Detect&#39; in ops: images.update({&#39;Detect&#39;: (image_detect, gamma)}) if &#39;Condense&#39; in ops: images.update({&#39;Condense&#39;: (image_condense, gamma)}) # Plot plt.figure(figsize=figsize) for i, title in enumerate(ops): image, gamma = images[title] plt.subplot(*subplot_shape, i+1) plt.imshow(tf.image.adjust_gamma(tf.squeeze(image), gamma)) plt.axis(&#39;off&#39;) plt.title(title) . from tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ layers.Conv2D(filters=64, kernel_size=3, strides=1, padding=&#39;same&#39;, activation=&#39;relu&#39;), layers.MaxPool2D(pool_size=2, strides=1, padding=&#39;same&#39;) # More layers follow ]) . import tensorflow as tf import matplotlib.pyplot as plt plt.rc(&#39;figure&#39;, autolayout=True) plt.rc(&#39;axes&#39;, labelweight=&#39;bold&#39;, labelsize=&#39;large&#39;, titleweight=&#39;bold&#39;, titlesize=18, titlepad=10) plt.rc(&#39;image&#39;, cmap=&#39;magma&#39;) image = circle([64, 64], val=1.0, r_shrink=3) image = tf.reshape(image, [*image.shape, 1]) # Bottom sobel kernel = tf.constant( [[-1, -2, -1], [0, 0, 0], [1, 2, 1]], ) show_kernel(kernel) . show_extraction( image, kernel, # Window parameters conv_stride=1, pool_size=2, pool_stride=2, subplot_shape=(1, 4), figsize=(14, 6), ) . show_extraction( image, kernel, # Window parameters conv_stride=3, pool_size=2, pool_stride=2, subplot_shape=(1, 4), figsize=(14, 6), ) . Custom Convnets . import os, warnings import matplotlib.pyplot as plt from matplotlib import gridspec import numpy as np import tensorflow as tf from tensorflow.keras.preprocessing import image_dataset_from_directory # Reproducability def set_seed(seed=31415): np.random.seed(seed) tf.random.set_seed(seed) os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed) os.environ[&#39;TF_DETERMINISTIC_OPS&#39;] = &#39;1&#39; set_seed() # Set Matplotlib defaults plt.rc(&#39;figure&#39;, autolayout=True) plt.rc(&#39;axes&#39;, labelweight=&#39;bold&#39;, labelsize=&#39;large&#39;, titleweight=&#39;bold&#39;, titlesize=18, titlepad=10) plt.rc(&#39;image&#39;, cmap=&#39;magma&#39;) warnings.filterwarnings(&quot;ignore&quot;) # to clean up output cells # Load training and validation sets ds_train_ = image_dataset_from_directory( &#39;/content/car-or-truck/train&#39;, labels=&#39;inferred&#39;, label_mode=&#39;binary&#39;, image_size=[128, 128], interpolation=&#39;nearest&#39;, batch_size=64, shuffle=True, ) ds_valid_ = image_dataset_from_directory( &#39;/content/car-or-truck/valid&#39;, labels=&#39;inferred&#39;, label_mode=&#39;binary&#39;, image_size=[128, 128], interpolation=&#39;nearest&#39;, batch_size=64, shuffle=False, ) # Data Pipeline def convert_to_float(image, label): image = tf.image.convert_image_dtype(image, dtype=tf.float32) return image, label AUTOTUNE = tf.data.experimental.AUTOTUNE ds_train = ( ds_train_ .map(convert_to_float) .cache() .prefetch(buffer_size=AUTOTUNE) ) ds_valid = ( ds_valid_ .map(convert_to_float) .cache() .prefetch(buffer_size=AUTOTUNE) ) . Found 5117 files belonging to 2 classes. Found 5051 files belonging to 2 classes. . from tensorflow import keras from tensorflow.keras import layers model = keras.Sequential([ # First Convolutional Block layers.Conv2D(filters=32, kernel_size=5, activation=&quot;relu&quot;, padding=&#39;same&#39;, # give the input dimensions in the first layer # [height, width, color channels(RGB)] input_shape=[128, 128, 3]), layers.MaxPool2D(), # Second Convolutional Block layers.Conv2D(filters=64, kernel_size=3, activation=&quot;relu&quot;, padding=&#39;same&#39;), layers.MaxPool2D(), # Third Convolutional Block layers.Conv2D(filters=128, kernel_size=3, activation=&quot;relu&quot;, padding=&#39;same&#39;), layers.MaxPool2D(), # Classifier Head layers.Flatten(), layers.Dense(units=6, activation=&quot;relu&quot;), layers.Dense(units=1, activation=&quot;sigmoid&quot;), ]) model.summary() . Model: &#34;sequential_5&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_5 (Conv2D) (None, 128, 128, 32) 2432 _________________________________________________________________ max_pooling2d_4 (MaxPooling2 (None, 64, 64, 32) 0 _________________________________________________________________ conv2d_6 (Conv2D) (None, 64, 64, 64) 18496 _________________________________________________________________ max_pooling2d_5 (MaxPooling2 (None, 32, 32, 64) 0 _________________________________________________________________ conv2d_7 (Conv2D) (None, 32, 32, 128) 73856 _________________________________________________________________ max_pooling2d_6 (MaxPooling2 (None, 16, 16, 128) 0 _________________________________________________________________ flatten (Flatten) (None, 32768) 0 _________________________________________________________________ dense (Dense) (None, 6) 196614 _________________________________________________________________ dense_1 (Dense) (None, 1) 7 ================================================================= Total params: 291,405 Trainable params: 291,405 Non-trainable params: 0 _________________________________________________________________ . model.compile( optimizer=tf.keras.optimizers.Adam(epsilon=0.01), loss=&#39;binary_crossentropy&#39;, metrics=[&#39;binary_accuracy&#39;] ) history = model.fit( ds_train, validation_data=ds_valid, epochs=40, ) . Epoch 1/40 80/80 [==============================] - 16s 176ms/step - loss: 0.6887 - binary_accuracy: 0.5486 - val_loss: 0.6714 - val_binary_accuracy: 0.5771 Epoch 2/40 80/80 [==============================] - 4s 49ms/step - loss: 0.6682 - binary_accuracy: 0.5873 - val_loss: 0.6577 - val_binary_accuracy: 0.6149 Epoch 3/40 80/80 [==============================] - 4s 50ms/step - loss: 0.6562 - binary_accuracy: 0.6085 - val_loss: 0.6503 - val_binary_accuracy: 0.6228 Epoch 4/40 80/80 [==============================] - 4s 49ms/step - loss: 0.6489 - binary_accuracy: 0.6200 - val_loss: 0.6431 - val_binary_accuracy: 0.6306 Epoch 5/40 80/80 [==============================] - 4s 50ms/step - loss: 0.6402 - binary_accuracy: 0.6367 - val_loss: 0.6326 - val_binary_accuracy: 0.6399 Epoch 6/40 80/80 [==============================] - 4s 50ms/step - loss: 0.6293 - binary_accuracy: 0.6527 - val_loss: 0.6229 - val_binary_accuracy: 0.6450 Epoch 7/40 80/80 [==============================] - 4s 50ms/step - loss: 0.6183 - binary_accuracy: 0.6615 - val_loss: 0.6177 - val_binary_accuracy: 0.6533 Epoch 8/40 80/80 [==============================] - 4s 50ms/step - loss: 0.6073 - binary_accuracy: 0.6736 - val_loss: 0.6174 - val_binary_accuracy: 0.6589 Epoch 9/40 80/80 [==============================] - 4s 51ms/step - loss: 0.5960 - binary_accuracy: 0.6838 - val_loss: 0.6102 - val_binary_accuracy: 0.6672 Epoch 10/40 80/80 [==============================] - 4s 50ms/step - loss: 0.5826 - binary_accuracy: 0.7020 - val_loss: 0.5941 - val_binary_accuracy: 0.6820 Epoch 11/40 80/80 [==============================] - 4s 50ms/step - loss: 0.5670 - binary_accuracy: 0.7159 - val_loss: 0.5693 - val_binary_accuracy: 0.6997 Epoch 12/40 80/80 [==============================] - 4s 50ms/step - loss: 0.5398 - binary_accuracy: 0.7355 - val_loss: 0.5462 - val_binary_accuracy: 0.7252 Epoch 13/40 80/80 [==============================] - 4s 51ms/step - loss: 0.5096 - binary_accuracy: 0.7603 - val_loss: 0.5255 - val_binary_accuracy: 0.7395 Epoch 14/40 80/80 [==============================] - 4s 51ms/step - loss: 0.4787 - binary_accuracy: 0.7830 - val_loss: 0.5272 - val_binary_accuracy: 0.7391 Epoch 15/40 80/80 [==============================] - 4s 51ms/step - loss: 0.4517 - binary_accuracy: 0.8059 - val_loss: 0.4909 - val_binary_accuracy: 0.7690 Epoch 16/40 80/80 [==============================] - 4s 51ms/step - loss: 0.4185 - binary_accuracy: 0.8191 - val_loss: 0.4803 - val_binary_accuracy: 0.7759 Epoch 17/40 80/80 [==============================] - 4s 51ms/step - loss: 0.3893 - binary_accuracy: 0.8372 - val_loss: 0.4811 - val_binary_accuracy: 0.7739 Epoch 18/40 80/80 [==============================] - 4s 51ms/step - loss: 0.3596 - binary_accuracy: 0.8512 - val_loss: 0.4906 - val_binary_accuracy: 0.7721 Epoch 19/40 80/80 [==============================] - 4s 51ms/step - loss: 0.3308 - binary_accuracy: 0.8655 - val_loss: 0.4955 - val_binary_accuracy: 0.7765 Epoch 20/40 80/80 [==============================] - 4s 51ms/step - loss: 0.2970 - binary_accuracy: 0.8806 - val_loss: 0.5094 - val_binary_accuracy: 0.7787 Epoch 21/40 80/80 [==============================] - 4s 51ms/step - loss: 0.2686 - binary_accuracy: 0.8925 - val_loss: 0.5316 - val_binary_accuracy: 0.7800 Epoch 22/40 80/80 [==============================] - 4s 51ms/step - loss: 0.2339 - binary_accuracy: 0.9074 - val_loss: 0.5685 - val_binary_accuracy: 0.7787 Epoch 23/40 80/80 [==============================] - 4s 51ms/step - loss: 0.2027 - binary_accuracy: 0.9194 - val_loss: 0.6088 - val_binary_accuracy: 0.7737 Epoch 24/40 80/80 [==============================] - 4s 51ms/step - loss: 0.2091 - binary_accuracy: 0.9093 - val_loss: 0.7433 - val_binary_accuracy: 0.7359 Epoch 25/40 80/80 [==============================] - 4s 51ms/step - loss: 0.2540 - binary_accuracy: 0.8911 - val_loss: 1.0190 - val_binary_accuracy: 0.6791 Epoch 26/40 80/80 [==============================] - 4s 51ms/step - loss: 0.2588 - binary_accuracy: 0.8905 - val_loss: 0.5047 - val_binary_accuracy: 0.8012 Epoch 27/40 80/80 [==============================] - 4s 52ms/step - loss: 0.1985 - binary_accuracy: 0.9178 - val_loss: 0.5351 - val_binary_accuracy: 0.7975 Epoch 28/40 80/80 [==============================] - 4s 51ms/step - loss: 0.1413 - binary_accuracy: 0.9465 - val_loss: 0.5278 - val_binary_accuracy: 0.8084 Epoch 29/40 80/80 [==============================] - 4s 51ms/step - loss: 0.1059 - binary_accuracy: 0.9659 - val_loss: 0.5548 - val_binary_accuracy: 0.8034 Epoch 30/40 80/80 [==============================] - 4s 51ms/step - loss: 0.1112 - binary_accuracy: 0.9622 - val_loss: 0.5590 - val_binary_accuracy: 0.7990 Epoch 31/40 80/80 [==============================] - 4s 51ms/step - loss: 0.1159 - binary_accuracy: 0.9579 - val_loss: 0.8036 - val_binary_accuracy: 0.7393 Epoch 32/40 80/80 [==============================] - 4s 51ms/step - loss: 0.1753 - binary_accuracy: 0.9256 - val_loss: 0.7633 - val_binary_accuracy: 0.7739 Epoch 33/40 80/80 [==============================] - 4s 51ms/step - loss: 0.1181 - binary_accuracy: 0.9484 - val_loss: 0.7871 - val_binary_accuracy: 0.7810 Epoch 34/40 80/80 [==============================] - 4s 51ms/step - loss: 0.0756 - binary_accuracy: 0.9755 - val_loss: 0.8611 - val_binary_accuracy: 0.7783 Epoch 35/40 80/80 [==============================] - 4s 52ms/step - loss: 0.0642 - binary_accuracy: 0.9798 - val_loss: 0.7956 - val_binary_accuracy: 0.7998 Epoch 36/40 80/80 [==============================] - 4s 52ms/step - loss: 0.0652 - binary_accuracy: 0.9768 - val_loss: 0.7117 - val_binary_accuracy: 0.8074 Epoch 37/40 80/80 [==============================] - 4s 52ms/step - loss: 0.0703 - binary_accuracy: 0.9735 - val_loss: 0.7860 - val_binary_accuracy: 0.7858 Epoch 38/40 80/80 [==============================] - 4s 52ms/step - loss: 0.0658 - binary_accuracy: 0.9777 - val_loss: 0.6763 - val_binary_accuracy: 0.8048 Epoch 39/40 80/80 [==============================] - 4s 52ms/step - loss: 0.0689 - binary_accuracy: 0.9772 - val_loss: 0.6997 - val_binary_accuracy: 0.7945 Epoch 40/40 80/80 [==============================] - 4s 51ms/step - loss: 0.0546 - binary_accuracy: 0.9814 - val_loss: 0.6994 - val_binary_accuracy: 0.7977 . import pandas as pd history_frame = pd.DataFrame(history.history) history_frame.loc[:, [&#39;loss&#39;, &#39;val_loss&#39;]].plot() history_frame.loc[:, [&#39;binary_accuracy&#39;, &#39;val_binary_accuracy&#39;]].plot(); . Data augmentation . from tensorflow import keras from tensorflow.keras import layers # these are a new feature in TF 2.2 from tensorflow.keras.layers.experimental import preprocessing pretrained_base = tf.keras.applications.VGG16(include_top=False, input_shape=(128, 128, 3)) pretrained_base.trainable = False model = keras.Sequential([ # Preprocessing preprocessing.RandomFlip(&#39;horizontal&#39;), # flip left-to-right preprocessing.RandomContrast(0.5), # contrast change by up to 50% # Base pretrained_base, # Head layers.Flatten(), layers.Dense(6, activation=&#39;relu&#39;), layers.Dense(1, activation=&#39;sigmoid&#39;), ]) . Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5 58892288/58889256 [==============================] - 0s 0us/step . model.compile( optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;binary_accuracy&#39;], ) history = model.fit( ds_train, validation_data=ds_valid, epochs=15, ) . Epoch 1/15 80/80 [==============================] - 23s 245ms/step - loss: 0.6309 - binary_accuracy: 0.6475 - val_loss: 0.5331 - val_binary_accuracy: 0.7632 Epoch 2/15 80/80 [==============================] - 15s 185ms/step - loss: 0.5260 - binary_accuracy: 0.7891 - val_loss: 0.4987 - val_binary_accuracy: 0.7945 Epoch 3/15 80/80 [==============================] - 15s 184ms/step - loss: 0.4521 - binary_accuracy: 0.8273 - val_loss: 0.3700 - val_binary_accuracy: 0.8472 Epoch 4/15 80/80 [==============================] - 15s 184ms/step - loss: 0.3684 - binary_accuracy: 0.8502 - val_loss: 0.3475 - val_binary_accuracy: 0.8529 Epoch 5/15 80/80 [==============================] - 15s 184ms/step - loss: 0.3419 - binary_accuracy: 0.8625 - val_loss: 0.3330 - val_binary_accuracy: 0.8606 Epoch 6/15 80/80 [==============================] - 15s 184ms/step - loss: 0.3224 - binary_accuracy: 0.8657 - val_loss: 0.3306 - val_binary_accuracy: 0.8584 Epoch 7/15 80/80 [==============================] - 15s 184ms/step - loss: 0.3089 - binary_accuracy: 0.8720 - val_loss: 0.3158 - val_binary_accuracy: 0.8666 Epoch 8/15 80/80 [==============================] - 15s 184ms/step - loss: 0.3046 - binary_accuracy: 0.8741 - val_loss: 0.3205 - val_binary_accuracy: 0.8648 Epoch 9/15 80/80 [==============================] - 15s 184ms/step - loss: 0.2743 - binary_accuracy: 0.8906 - val_loss: 0.3124 - val_binary_accuracy: 0.8670 Epoch 10/15 80/80 [==============================] - 15s 184ms/step - loss: 0.2704 - binary_accuracy: 0.8898 - val_loss: 0.3032 - val_binary_accuracy: 0.8739 Epoch 11/15 80/80 [==============================] - 15s 184ms/step - loss: 0.2564 - binary_accuracy: 0.8982 - val_loss: 0.3021 - val_binary_accuracy: 0.8749 Epoch 12/15 80/80 [==============================] - 15s 184ms/step - loss: 0.2696 - binary_accuracy: 0.8891 - val_loss: 0.3006 - val_binary_accuracy: 0.8747 Epoch 13/15 80/80 [==============================] - 15s 185ms/step - loss: 0.2485 - binary_accuracy: 0.8932 - val_loss: 0.3006 - val_binary_accuracy: 0.8759 Epoch 14/15 80/80 [==============================] - 15s 184ms/step - loss: 0.2484 - binary_accuracy: 0.8961 - val_loss: 0.3011 - val_binary_accuracy: 0.8747 Epoch 15/15 80/80 [==============================] - 15s 184ms/step - loss: 0.2417 - binary_accuracy: 0.8989 - val_loss: 0.2937 - val_binary_accuracy: 0.8771 . import pandas as pd history_frame = pd.DataFrame(history.history) history_frame.loc[:, [&#39;loss&#39;, &#39;val_loss&#39;]].plot() history_frame.loc[:, [&#39;binary_accuracy&#39;, &#39;val_binary_accuracy&#39;]].plot(); .",
            "url": "https://lenguist.github.io/site/2021/03/13/computer-vision.html",
            "relUrl": "/2021/03/13/computer-vision.html",
            "date": " • Mar 13, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Seaborn",
            "content": ". Hello, Seaborn . import pandas as pd pd.plotting.register_matplotlib_converters() import matplotlib.pyplot as plt import seaborn as sns . fifa_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/fifa.csv&quot; # Read the file into a variable fifa_data fifa_data = pd.read_csv(fifa_filepath, index_col=&quot;Date&quot;, parse_dates=True) # Set the width and height of the figure plt.figure(figsize=(16,6)) # Line chart showing how FIFA rankings evolved over time sns.lineplot(data=fifa_data) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa69ac0e0d0&gt; . Line Charts . spotify_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/spotify.csv&quot; # Read the file into a variable spotify_data spotify_data = pd.read_csv(spotify_filepath, index_col=&quot;Date&quot;, parse_dates=True) # Set the width and height of the figure plt.figure(figsize=(14,6)) # Add title plt.title(&quot;Daily Global Streams of Popular Songs in 2017-2018&quot;) sns.lineplot(data=spotify_data) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa69ac6fdd0&gt; . plt.figure(figsize=(14,6)) # Add title plt.title(&quot;Daily Global Streams of Popular Songs in 2017-2018&quot;) # Line chart showing daily global streams of &#39;Shape of You&#39; sns.lineplot(data=spotify_data[&#39;Shape of You&#39;], label=&quot;Shape of You&quot;) # Line chart showing daily global streams of &#39;Despacito&#39; sns.lineplot(data=spotify_data[&#39;Despacito&#39;], label=&quot;Despacito&quot;) # Add label for horizontal axis plt.xlabel(&quot;Date&quot;) . Text(0.5, 0, &#39;Date&#39;) . Bar Charts and Heatmaps . flight_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/flight_delays.csv&quot; # Read the file into a variable flight_data flight_data = pd.read_csv(flight_filepath, index_col=&quot;Month&quot;) . plt.figure(figsize=(10,6)) # Add title plt.title(&quot;Average Arrival Delay for Spirit Airlines Flights, by Month&quot;) # Bar chart showing average arrival delay for Spirit Airlines flights by month sns.barplot(x=flight_data.index, y=flight_data[&#39;NK&#39;]) # Add label for vertical axis plt.ylabel(&quot;Arrival delay (in minutes)&quot;) . Text(0, 0.5, &#39;Arrival delay (in minutes)&#39;) . plt.figure(figsize=(14,7)) # Add title plt.title(&quot;Average Arrival Delay for Each Airline, by Month&quot;) # Heatmap showing average arrival delay for each airline by month sns.heatmap(data=flight_data, annot=True) # Add label for horizontal axis plt.xlabel(&quot;Airline&quot;) . Text(0.5, 42.0, &#39;Airline&#39;) . Scatter Plots . insurance_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/insurance.csv&quot; # Read the file into a variable insurance_data insurance_data = pd.read_csv(insurance_filepath) . sns.scatterplot(x=insurance_data[&#39;bmi&#39;], y=insurance_data[&#39;charges&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa69af65c50&gt; . sns.regplot(x=insurance_data[&#39;bmi&#39;], y=insurance_data[&#39;charges&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa691cc0f90&gt; . sns.scatterplot(x=insurance_data[&#39;bmi&#39;], y=insurance_data[&#39;charges&#39;], hue=insurance_data[&#39;smoker&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa691b73650&gt; . sns.lmplot(x=&quot;bmi&quot;, y=&quot;charges&quot;, hue=&quot;smoker&quot;, data=insurance_data) . &lt;seaborn.axisgrid.FacetGrid at 0x7fa68d375d10&gt; . sns.swarmplot(x=insurance_data[&#39;smoker&#39;], y=insurance_data[&#39;charges&#39;]) . /usr/local/lib/python3.7/dist-packages/seaborn/categorical.py:1296: UserWarning: 67.3% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa68d286c10&gt; . Distributions . iris_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/iris.csv&quot; # Read the file into a variable iris_data iris_data = pd.read_csv(iris_filepath, index_col=&quot;Id&quot;) # Print the first 5 rows of the data iris_data.head() . Sepal Length (cm) Sepal Width (cm) Petal Length (cm) Petal Width (cm) Species . Id . 1 5.1 | 3.5 | 1.4 | 0.2 | Iris-setosa | . 2 4.9 | 3.0 | 1.4 | 0.2 | Iris-setosa | . 3 4.7 | 3.2 | 1.3 | 0.2 | Iris-setosa | . 4 4.6 | 3.1 | 1.5 | 0.2 | Iris-setosa | . 5 5.0 | 3.6 | 1.4 | 0.2 | Iris-setosa | . sns.distplot(a=iris_data[&#39;Petal Length (cm)&#39;], kde=False) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa68d1b5990&gt; . sns.kdeplot(data=iris_data[&#39;Petal Length (cm)&#39;], shade=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa68d12ec10&gt; . sns.jointplot(x=iris_data[&#39;Petal Length (cm)&#39;], y=iris_data[&#39;Sepal Width (cm)&#39;], kind=&quot;kde&quot;, shade=True) . &lt;seaborn.axisgrid.JointGrid at 0x7fa68cd39f50&gt; . iris_set_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/iris_setosa.csv&quot; iris_ver_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/iris_versicolor.csv&quot; iris_vir_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/iris_virginica.csv&quot; # Read the files into variables iris_set_data = pd.read_csv(iris_set_filepath, index_col=&quot;Id&quot;) iris_ver_data = pd.read_csv(iris_ver_filepath, index_col=&quot;Id&quot;) iris_vir_data = pd.read_csv(iris_vir_filepath, index_col=&quot;Id&quot;) . sns.distplot(a=iris_set_data[&#39;Petal Length (cm)&#39;], label=&quot;Iris-setosa&quot;, kde=False) sns.distplot(a=iris_ver_data[&#39;Petal Length (cm)&#39;], label=&quot;Iris-versicolor&quot;, kde=False) sns.distplot(a=iris_vir_data[&#39;Petal Length (cm)&#39;], label=&quot;Iris-virginica&quot;, kde=False) # Add title plt.title(&quot;Histogram of Petal Lengths, by Species&quot;) # Force legend to appear plt.legend() . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.legend.Legend at 0x7fa68d0e7150&gt; . sns.kdeplot(data=iris_set_data[&#39;Petal Length (cm)&#39;], label=&quot;Iris-setosa&quot;, shade=True) sns.kdeplot(data=iris_ver_data[&#39;Petal Length (cm)&#39;], label=&quot;Iris-versicolor&quot;, shade=True) sns.kdeplot(data=iris_vir_data[&#39;Petal Length (cm)&#39;], label=&quot;Iris-virginica&quot;, shade=True) # Add title plt.title(&quot;Distribution of Petal Lengths, by Species&quot;) . Text(0.5, 1.0, &#39;Distribution of Petal Lengths, by Species&#39;) . Choosing Plot Types and Custom Styles . plt.figure(figsize=(12,6)) sns.lineplot(data=spotify_data) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa68d55da90&gt; . sns.set_style(&quot;dark&quot;) # Line chart plt.figure(figsize=(12,6)) sns.lineplot(data=spotify_data) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa68cad4750&gt; .",
            "url": "https://lenguist.github.io/site/kaggle/2021/03/12/seaborn.html",
            "relUrl": "/kaggle/2021/03/12/seaborn.html",
            "date": " • Mar 12, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Neural network from scratch",
            "content": ". I am currently on a plane from Frankfurt to Newark, and I can&#39;t fall asleep (probably because of the 2 cups of cofee I drank). So here is a challenge: I need to write a neural network from scratch, using only basic libraries (pandas, numpy, matplotlib). No internet, no tutorials, only me, libs documentation, an abundance of free time and existential dread of being in a metal box 12 km above the Atlantic. Let&#39;s fucking go. . Okay so first thigns first what the hell am I trying to do here. I initially tried to do smth based on the datasets I have downloaded, but all of them are fro text analysis and I soundly decided that coding embeddings from scratch would not be particularly fun (though maybe one day...). NB: Apparently I did have some downloaded from the kaggle visualization course, but when I found that out I pivoted anyway so duh. NB2: Actually coding rnn from scratch sounds really fun. Definitely should do it some day. . So instead I will be generating some synthetic data. Initially I wanted to do classification cause regressions is boring, but after delving deeper into the task I understood I grossly underestimated how hard it is to do ANYTHING with those basic libs, so I won&#39;t be too picky. First I wanted to fit a quadratic, but then when I understood I will have to do backprop by hand I decided to hell with it I will do the bare minimum which works. So yeah. I will generate some noise linear dataset, do a very small classic network (perceptron), and try to make it work. Somehow. . Setup . import numpy as np import matplotlib as plt import pandas as pd . Generating data for the regression . Okay this was actually harder than I though cause random functions in numpy dont work the way you woudl expect. Apparently you can&#39;t generate random reals only in range (0,1), so I did some quick fixes to circumvent that. But overall, it actually works! Unbelievable . data = [] for i in range(1000): x = np.random.rand()*20 noise = np.random.randn() y = 6.9*(x+noise) + 6.9*noise #nice. data.append([x,y]) import matplotlib as plt regression_data = pd.DataFrame(data) regression_data.plot(x=0, y=1, kind=&#39;scatter&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc4a768b910&gt; . Helper functions . Apparently numpy is even more limited than I thought. Or maybe I am a dumbass and can&#39;t find the right funcs. Whatever. Defined some helpers functions used later on . def sigmoid(x): return 1/(1+np.exp(-x)) def d_sigmoid(x): return np.exp(-x)/((1+np.exp(-x))**2) def random_sign(x): if np.random.rand()&gt;0.5: det = 1 else: det = -1 return x*det def initialize(): return random_sign(np.random.randint(10)/10) . Model . Okaaaay. I scratched my head for a while here. I totally forgot how exactly back prop works. Also apprently I forgot calculust and linear algebra. Took me like half an hour to recall how derivatives work and how matrix multiplication works. But in the end, I got the right idea. The problem is, the elegant and simple layer-as-a-matrix-multiplication approach does not work cause numpy has no gradient, nothing. And defining gradient from scratch means I need a way to access individual derivatives ... . NB: actually there should have been a way for me to do this elegantly either way. As it turned out later operations performed for each weight and each layer are really similar and could have been written as applying some transformation to an array. I should revise it some day, probably. . So anyway. After some thinking I decided to abandon elegance and go for something which, well, I can make work. So instead of treating weights as matrices, and neurons and biases as vectors, I treat everything as a number, plain and a simple. The problem is ... that&#39;s a lot of numbers. Even for a modest 1-8-8-8-1 network that&#39;s almost 200 weights and 25 biases. So I scaled my ambitions back a bit to a 1-2-3-1 network. I am ot even sure it would be enough to fit a quadratic, but my data is linear so it should in theory at least handle it. I ended up with 11 weights and 6 biases. I will be using logistic sigmoid for my activation layer, and I will apply for the hidden layers only. . ################################################################################# #Notation #for weights: #first number - layer, second number - input neuron, third number - output neuron #for biases: #first number-layer, second number - neuron ################################################################################# class myNetwork(object): def __init__(self): #Initializing weights #First layer self.w111=initialize() self.w112=initialize() self.b11=initialize() self.b12=initialize() #Second layer self.w211=initialize() self.w221=initialize() self.w212=initialize() self.w222=initialize() self.w213=initialize() self.w223=initialize() self.b21=initialize() self.b22=initialize() self.b23=initialize() #Third layer self.w311=initialize() self.w321=initialize() self.w331=initialize() self.b31=initialize() #accepts a list in the form of [x,y] def forward(self, datapoint, predict=False): self.n0 = datapoint[0] #First layer self.n11 = sigmoid(self.w111*self.n0+self.b11) self.n12 = sigmoid(self.w112*self.n0+self.b12) #Second layer self.n21 = sigmoid(self.w211*self.n11+self.w221*self.n12+self.b21) self.n22 = sigmoid(self.w212*self.n11+self.w222*self.n12+self.b22) self.n23 = sigmoid(self.w213*self.n11+self.w223*self.n12+self.b23) #Third layer self.n31 = self.w311*self.n21+self.w321*self.n22+self.w331*self.n23+self.b31 out = self.n31 if predict: return out else: #Computing loss #loss function loss = (out-datapoint[1])**2 return out, loss def backpropagate(self,datapoint, out, loss): #Backpropagation #d_# denotes a derivative of loss with respect to # d_out = 2*out - 2*datapoint[1] #out = w311*n21+w321*n22+w331*n23+b31 self.d_w311 = d_out*self.n21 self.d_w321 = d_out*self.n22 self.d_w331 = d_out*self.n23 self.d_b31 = d_out d_n21 = d_out*self.w311 d_n22 = d_out*self.w321 d_n23 = d_out*self.w331 #n21 = sigmoid(w211*n11+w221*n12+b21) d_inner_n21 = d_n21*d_sigmoid(self.w211*self.n11+self.w221*self.n12+self.b21) self.d_w211 = d_inner_n21*self.n11 self.d_w221 = d_inner_n21*self.n12 self.d_b21 = d_inner_n21 d_n21_n11 = d_inner_n21*self.d_w211 d_n21_n12 = d_inner_n21*self.d_w221 #n22 = sigmoid(w212*n11+w222*n12+b22) d_inner_n22 = d_n22*d_sigmoid(self.w212*self.n11+self.w222*self.n12+self.b22) self.d_w212 = d_inner_n22*self.n11 self.d_w222 = d_inner_n22*self.n12 self.d_b22 = d_inner_n22 d_n22_n11 = d_inner_n22*self.d_w212 d_n22_n12 = d_inner_n22*self.d_w222 #n23 = sigmoid(w213*n11+w223*n12+b23) d_inner_n23 = d_n23*d_sigmoid(self.w213*self.n11+self.w223*self.n12+self.b23) self.d_w213 = d_inner_n23*self.n11 self.d_w223 = d_inner_n23*self.n12 self.d_b23 = d_inner_n23 d_n23_n11 = d_inner_n21*self.d_w213 d_n23_n12 = d_inner_n21*self.d_w223 ############################## d_n11 = d_n21_n11+d_n22_n11+d_n23_n11 d_n12 = d_n21_n12+d_n22_n12+d_n23_n12 #n11 = sigmoid(w111*n0+b11) d_inner_n11 = d_n11*d_sigmoid(self.w111*self.n0+self.b11) self.d_w111 = d_inner_n11*self.n0 self.d_b11 = d_inner_n11 #n12 = sigmoid(w112*n0+b12) d_inner_n12 = d_n12*d_sigmoid(self.w112*self.n0+self.b12) self.d_w112 = d_inner_n12*self.n0 self.d_b12 = d_inner_n12 def update_weights(self,lr): #I will be using standard SGD, so no fancy optimizers and schedulers. #Update params #First layer self.w111 -= self.d_w111*lr self.w112 -= self.d_w112*lr self.b11 -= self.d_b11*lr self.b12 -= self.d_b12*lr #Second layer self.w211 -= self.d_w211*lr self.w221 -= self.d_w221*lr self.w212 -= self.d_w212*lr self.w222 -= self.d_w222*lr self.w213 -= self.d_w213*lr self.w223 -= self.d_w223*lr self.b21 -= self.d_b21*lr self.b22 -= self.d_b22*lr self.b23 -= self.d_b23*lr #Third layer self.w311 -= self.d_w311*lr self.w321 -= self.d_w321*lr self.w331 -= self.d_w331*lr def train(self, dataset, n_epochs, lr): losses = [] for i in range(n_epochs): epoch_loss = 0 i += 1 #print(&quot;=&quot;*20) #print(&quot;Performing epoch &quot; + str(i)) for datapoint in dataset: out, loss = self.forward(datapoint) epoch_loss+=loss self.backpropagate(datapoint,out,loss) self.update_weights(lr) losses.append(epoch_loss) #Omitting first epoch loss cause that epoch is just shit return pd.DataFrame(losses[1:]).plot() def predict(self, x): out = self.forward([x], predict=True) return out . Is this good code? No. Does it work? No actually:) I think my network might be too small for the task. whatever) It compiles))) Here are the results: . net = myNetwork() . net.train(dataset=data, n_epochs=47, lr=0.01) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc4a7783610&gt; . As you can see the loss goes down the way you would expect it to. Which is nice. But also it&#39;s kinda high in the end ... Here is why ... . Input data . regression_data = pd.DataFrame(data) regression_data.plot(x=0, y=1, kind=&#39;scatter&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc4a7886850&gt; . Output data . points = [] for i in range(0,20): points.append(net.predict(i)) points = pd.DataFrame(points) points.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc4a7aaa750&gt; . I mean... Theya are not even close))))))) But it does train and it is kinda close -- the problem is not in the model itself, but more in the model-dataset fit. Also, I did not use any regularization, so that may affect stuff. Regardless ... It was a fun experience! Hopefully I can come back to this one day and fix it for good))) .",
            "url": "https://lenguist.github.io/site/from_scratch/2021/03/12/nn-from-scratch.html",
            "relUrl": "/from_scratch/2021/03/12/nn-from-scratch.html",
            "date": " • Mar 12, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://lenguist.github.io/site/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "AI researcher and developer from Ukraine. Currently studying at Lawrenceville school, NJ, USA. .",
          "url": "https://lenguist.github.io/site/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://lenguist.github.io/site/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}