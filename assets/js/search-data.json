{
  
    
        "post0": {
            "title": "Seaborn",
            "content": ". Hello, Seaborn . import pandas as pd pd.plotting.register_matplotlib_converters() import matplotlib.pyplot as plt import seaborn as sns . fifa_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/fifa.csv&quot; # Read the file into a variable fifa_data fifa_data = pd.read_csv(fifa_filepath, index_col=&quot;Date&quot;, parse_dates=True) # Set the width and height of the figure plt.figure(figsize=(16,6)) # Line chart showing how FIFA rankings evolved over time sns.lineplot(data=fifa_data) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa69ac0e0d0&gt; . Line Charts . spotify_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/spotify.csv&quot; # Read the file into a variable spotify_data spotify_data = pd.read_csv(spotify_filepath, index_col=&quot;Date&quot;, parse_dates=True) # Set the width and height of the figure plt.figure(figsize=(14,6)) # Add title plt.title(&quot;Daily Global Streams of Popular Songs in 2017-2018&quot;) sns.lineplot(data=spotify_data) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa69ac6fdd0&gt; . plt.figure(figsize=(14,6)) # Add title plt.title(&quot;Daily Global Streams of Popular Songs in 2017-2018&quot;) # Line chart showing daily global streams of &#39;Shape of You&#39; sns.lineplot(data=spotify_data[&#39;Shape of You&#39;], label=&quot;Shape of You&quot;) # Line chart showing daily global streams of &#39;Despacito&#39; sns.lineplot(data=spotify_data[&#39;Despacito&#39;], label=&quot;Despacito&quot;) # Add label for horizontal axis plt.xlabel(&quot;Date&quot;) . Text(0.5, 0, &#39;Date&#39;) . Bar Charts and Heatmaps . flight_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/flight_delays.csv&quot; # Read the file into a variable flight_data flight_data = pd.read_csv(flight_filepath, index_col=&quot;Month&quot;) . plt.figure(figsize=(10,6)) # Add title plt.title(&quot;Average Arrival Delay for Spirit Airlines Flights, by Month&quot;) # Bar chart showing average arrival delay for Spirit Airlines flights by month sns.barplot(x=flight_data.index, y=flight_data[&#39;NK&#39;]) # Add label for vertical axis plt.ylabel(&quot;Arrival delay (in minutes)&quot;) . Text(0, 0.5, &#39;Arrival delay (in minutes)&#39;) . plt.figure(figsize=(14,7)) # Add title plt.title(&quot;Average Arrival Delay for Each Airline, by Month&quot;) # Heatmap showing average arrival delay for each airline by month sns.heatmap(data=flight_data, annot=True) # Add label for horizontal axis plt.xlabel(&quot;Airline&quot;) . Text(0.5, 42.0, &#39;Airline&#39;) . Scatter Plots . insurance_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/insurance.csv&quot; # Read the file into a variable insurance_data insurance_data = pd.read_csv(insurance_filepath) . sns.scatterplot(x=insurance_data[&#39;bmi&#39;], y=insurance_data[&#39;charges&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa69af65c50&gt; . sns.regplot(x=insurance_data[&#39;bmi&#39;], y=insurance_data[&#39;charges&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa691cc0f90&gt; . sns.scatterplot(x=insurance_data[&#39;bmi&#39;], y=insurance_data[&#39;charges&#39;], hue=insurance_data[&#39;smoker&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa691b73650&gt; . sns.lmplot(x=&quot;bmi&quot;, y=&quot;charges&quot;, hue=&quot;smoker&quot;, data=insurance_data) . &lt;seaborn.axisgrid.FacetGrid at 0x7fa68d375d10&gt; . sns.swarmplot(x=insurance_data[&#39;smoker&#39;], y=insurance_data[&#39;charges&#39;]) . /usr/local/lib/python3.7/dist-packages/seaborn/categorical.py:1296: UserWarning: 67.3% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa68d286c10&gt; . Distributions . iris_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/iris.csv&quot; # Read the file into a variable iris_data iris_data = pd.read_csv(iris_filepath, index_col=&quot;Id&quot;) # Print the first 5 rows of the data iris_data.head() . Sepal Length (cm) Sepal Width (cm) Petal Length (cm) Petal Width (cm) Species . Id . 1 5.1 | 3.5 | 1.4 | 0.2 | Iris-setosa | . 2 4.9 | 3.0 | 1.4 | 0.2 | Iris-setosa | . 3 4.7 | 3.2 | 1.3 | 0.2 | Iris-setosa | . 4 4.6 | 3.1 | 1.5 | 0.2 | Iris-setosa | . 5 5.0 | 3.6 | 1.4 | 0.2 | Iris-setosa | . sns.distplot(a=iris_data[&#39;Petal Length (cm)&#39;], kde=False) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa68d1b5990&gt; . sns.kdeplot(data=iris_data[&#39;Petal Length (cm)&#39;], shade=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa68d12ec10&gt; . sns.jointplot(x=iris_data[&#39;Petal Length (cm)&#39;], y=iris_data[&#39;Sepal Width (cm)&#39;], kind=&quot;kde&quot;, shade=True) . &lt;seaborn.axisgrid.JointGrid at 0x7fa68cd39f50&gt; . iris_set_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/iris_setosa.csv&quot; iris_ver_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/iris_versicolor.csv&quot; iris_vir_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/iris_virginica.csv&quot; # Read the files into variables iris_set_data = pd.read_csv(iris_set_filepath, index_col=&quot;Id&quot;) iris_ver_data = pd.read_csv(iris_ver_filepath, index_col=&quot;Id&quot;) iris_vir_data = pd.read_csv(iris_vir_filepath, index_col=&quot;Id&quot;) . sns.distplot(a=iris_set_data[&#39;Petal Length (cm)&#39;], label=&quot;Iris-setosa&quot;, kde=False) sns.distplot(a=iris_ver_data[&#39;Petal Length (cm)&#39;], label=&quot;Iris-versicolor&quot;, kde=False) sns.distplot(a=iris_vir_data[&#39;Petal Length (cm)&#39;], label=&quot;Iris-virginica&quot;, kde=False) # Add title plt.title(&quot;Histogram of Petal Lengths, by Species&quot;) # Force legend to appear plt.legend() . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.legend.Legend at 0x7fa68d0e7150&gt; . sns.kdeplot(data=iris_set_data[&#39;Petal Length (cm)&#39;], label=&quot;Iris-setosa&quot;, shade=True) sns.kdeplot(data=iris_ver_data[&#39;Petal Length (cm)&#39;], label=&quot;Iris-versicolor&quot;, shade=True) sns.kdeplot(data=iris_vir_data[&#39;Petal Length (cm)&#39;], label=&quot;Iris-virginica&quot;, shade=True) # Add title plt.title(&quot;Distribution of Petal Lengths, by Species&quot;) . Text(0.5, 1.0, &#39;Distribution of Petal Lengths, by Species&#39;) . Choosing Plot Types and Custom Styles . plt.figure(figsize=(12,6)) sns.lineplot(data=spotify_data) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa68d55da90&gt; . sns.set_style(&quot;dark&quot;) # Line chart plt.figure(figsize=(12,6)) sns.lineplot(data=spotify_data) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa68cad4750&gt; .",
            "url": "https://lenguist.github.io/site/kaggle/2021/03/12/seaborn.html",
            "relUrl": "/kaggle/2021/03/12/seaborn.html",
            "date": " • Mar 12, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Neural network from scratch",
            "content": ". I am currently on a plane from Frankfurt to Newark, and I can&#39;t fall asleep (probably because of the 2 cups of cofee I drank). So here is a challenge: I need to write a neural network from scratch, using only basic libraries (pandas, numpy, matplotlib). No internet, no tutorials, only me, libs documentation, an abundance of free time and existential dread of being in a metal box 12 km above the Atlantic. Let&#39;s fucking go. . Okay so first thigns first what the hell am I trying to do here. I initially tried to do smth based on the datasets I have downloaded, but all of them are fro text analysis and I soundly decided that coding embeddings from scratch would not be particularly fun (though maybe one day...). NB: Apparently I did have some downloaded from the kaggle visualization course, but when I found that out I pivoted anyway so duh. NB2: Actually coding rnn from scratch sounds really fun. Definitely should do it some day. . So instead I will be generating some synthetic data. Initially I wanted to do classification cause regressions is boring, but after delving deeper into the task I understood I grossly underestimated how hard it is to do ANYTHING with those basic libs, so I won&#39;t be too picky. First I wanted to fit a quadratic, but then when I understood I will have to do backprop by hand I decided to hell with it I will do the bare minimum which works. So yeah. I will generate some noise linear dataset, do a very small classic network (perceptron), and try to make it work. Somehow. . Setup . import numpy as np import matplotlib as plt import pandas as pd . Generating data for the regression . Okay this was actually harder than I though cause random functions in numpy dont work the way you woudl expect. Apparently you can&#39;t generate random reals only in range (0,1), so I did some quick fixes to circumvent that. But overall, it actually works! Unbelievable . data = [] for i in range(1000): x = np.random.rand()*20 noise = np.random.randn() y = 6.9*(x+noise) + 6.9*noise #nice. data.append([x,y]) import matplotlib as plt regression_data = pd.DataFrame(data) regression_data.plot(x=0, y=1, kind=&#39;scatter&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc4a768b910&gt; . Helper functions . Apparently numpy is even more limited than I thought. Or maybe I am a dumbass and can&#39;t find the right funcs. Whatever. Defined some helpers functions used later on . def sigmoid(x): return 1/(1+np.exp(-x)) def d_sigmoid(x): return np.exp(-x)/((1+np.exp(-x))**2) def random_sign(x): if np.random.rand()&gt;0.5: det = 1 else: det = -1 return x*det def initialize(): return random_sign(np.random.randint(10)/10) . Model . Okaaaay. I scratched my head for a while here. I totally forgot how exactly back prop works. Also apprently I forgot calculust and linear algebra. Took me like half an hour to recall how derivatives work and how matrix multiplication works. But in the end, I got the right idea. The problem is, the elegant and simple layer-as-a-matrix-multiplication approach does not work cause numpy has no gradient, nothing. And defining gradient from scratch means I need a way to access individual derivatives ... . NB: actually there should have been a way for me to do this elegantly either way. As it turned out later operations performed for each weight and each layer are really similar and could have been written as applying some transformation to an array. I should revise it some day, probably. . So anyway. After some thinking I decided to abandon elegance and go for something which, well, I can make work. So instead of treating weights as matrices, and neurons and biases as vectors, I treat everything as a number, plain and a simple. The problem is ... that&#39;s a lot of numbers. Even for a modest 1-8-8-8-1 network that&#39;s almost 200 weights and 25 biases. So I scaled my ambitions back a bit to a 1-2-3-1 network. I am ot even sure it would be enough to fit a quadratic, but my data is linear so it should in theory at least handle it. I ended up with 11 weights and 6 biases. I will be using logistic sigmoid for my activation layer, and I will apply for the hidden layers only. . ################################################################################# #Notation #for weights: #first number - layer, second number - input neuron, third number - output neuron #for biases: #first number-layer, second number - neuron ################################################################################# class myNetwork(object): def __init__(self): #Initializing weights #First layer self.w111=initialize() self.w112=initialize() self.b11=initialize() self.b12=initialize() #Second layer self.w211=initialize() self.w221=initialize() self.w212=initialize() self.w222=initialize() self.w213=initialize() self.w223=initialize() self.b21=initialize() self.b22=initialize() self.b23=initialize() #Third layer self.w311=initialize() self.w321=initialize() self.w331=initialize() self.b31=initialize() #accepts a list in the form of [x,y] def forward(self, datapoint, predict=False): self.n0 = datapoint[0] #First layer self.n11 = sigmoid(self.w111*self.n0+self.b11) self.n12 = sigmoid(self.w112*self.n0+self.b12) #Second layer self.n21 = sigmoid(self.w211*self.n11+self.w221*self.n12+self.b21) self.n22 = sigmoid(self.w212*self.n11+self.w222*self.n12+self.b22) self.n23 = sigmoid(self.w213*self.n11+self.w223*self.n12+self.b23) #Third layer self.n31 = self.w311*self.n21+self.w321*self.n22+self.w331*self.n23+self.b31 out = self.n31 if predict: return out else: #Computing loss #loss function loss = (out-datapoint[1])**2 return out, loss def backpropagate(self,datapoint, out, loss): #Backpropagation #d_# denotes a derivative of loss with respect to # d_out = 2*out - 2*datapoint[1] #out = w311*n21+w321*n22+w331*n23+b31 self.d_w311 = d_out*self.n21 self.d_w321 = d_out*self.n22 self.d_w331 = d_out*self.n23 self.d_b31 = d_out d_n21 = d_out*self.w311 d_n22 = d_out*self.w321 d_n23 = d_out*self.w331 #n21 = sigmoid(w211*n11+w221*n12+b21) d_inner_n21 = d_n21*d_sigmoid(self.w211*self.n11+self.w221*self.n12+self.b21) self.d_w211 = d_inner_n21*self.n11 self.d_w221 = d_inner_n21*self.n12 self.d_b21 = d_inner_n21 d_n21_n11 = d_inner_n21*self.d_w211 d_n21_n12 = d_inner_n21*self.d_w221 #n22 = sigmoid(w212*n11+w222*n12+b22) d_inner_n22 = d_n22*d_sigmoid(self.w212*self.n11+self.w222*self.n12+self.b22) self.d_w212 = d_inner_n22*self.n11 self.d_w222 = d_inner_n22*self.n12 self.d_b22 = d_inner_n22 d_n22_n11 = d_inner_n22*self.d_w212 d_n22_n12 = d_inner_n22*self.d_w222 #n23 = sigmoid(w213*n11+w223*n12+b23) d_inner_n23 = d_n23*d_sigmoid(self.w213*self.n11+self.w223*self.n12+self.b23) self.d_w213 = d_inner_n23*self.n11 self.d_w223 = d_inner_n23*self.n12 self.d_b23 = d_inner_n23 d_n23_n11 = d_inner_n21*self.d_w213 d_n23_n12 = d_inner_n21*self.d_w223 ############################## d_n11 = d_n21_n11+d_n22_n11+d_n23_n11 d_n12 = d_n21_n12+d_n22_n12+d_n23_n12 #n11 = sigmoid(w111*n0+b11) d_inner_n11 = d_n11*d_sigmoid(self.w111*self.n0+self.b11) self.d_w111 = d_inner_n11*self.n0 self.d_b11 = d_inner_n11 #n12 = sigmoid(w112*n0+b12) d_inner_n12 = d_n12*d_sigmoid(self.w112*self.n0+self.b12) self.d_w112 = d_inner_n12*self.n0 self.d_b12 = d_inner_n12 def update_weights(self,lr): #I will be using standard SGD, so no fancy optimizers and schedulers. #Update params #First layer self.w111 -= self.d_w111*lr self.w112 -= self.d_w112*lr self.b11 -= self.d_b11*lr self.b12 -= self.d_b12*lr #Second layer self.w211 -= self.d_w211*lr self.w221 -= self.d_w221*lr self.w212 -= self.d_w212*lr self.w222 -= self.d_w222*lr self.w213 -= self.d_w213*lr self.w223 -= self.d_w223*lr self.b21 -= self.d_b21*lr self.b22 -= self.d_b22*lr self.b23 -= self.d_b23*lr #Third layer self.w311 -= self.d_w311*lr self.w321 -= self.d_w321*lr self.w331 -= self.d_w331*lr def train(self, dataset, n_epochs, lr): losses = [] for i in range(n_epochs): epoch_loss = 0 i += 1 #print(&quot;=&quot;*20) #print(&quot;Performing epoch &quot; + str(i)) for datapoint in dataset: out, loss = self.forward(datapoint) epoch_loss+=loss self.backpropagate(datapoint,out,loss) self.update_weights(lr) losses.append(epoch_loss) #Omitting first epoch loss cause that epoch is just shit return pd.DataFrame(losses[1:]).plot() def predict(self, x): out = self.forward([x], predict=True) return out . Is this good code? No. Does it work? No actually:) I think my network might be too small for the task. whatever) It compiles))) Here are the results: . net = myNetwork() . net.train(dataset=data, n_epochs=47, lr=0.01) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc4a7783610&gt; . As you can see the loss goes down the way you would expect it to. Which is nice. But also it&#39;s kinda high in the end ... Here is why ... . Input data . regression_data = pd.DataFrame(data) regression_data.plot(x=0, y=1, kind=&#39;scatter&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc4a7886850&gt; . Output data . points = [] for i in range(0,20): points.append(net.predict(i)) points = pd.DataFrame(points) points.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc4a7aaa750&gt; . I mean... Theya are not even close))))))) But it does train and it is kinda close -- the problem is not in the model itself, but more in the model-dataset fit. Also, I did not use any regularization, so that may affect stuff. Regardless ... It was a fun experience! Hopefully I can come back to this one day and fix it for good))) .",
            "url": "https://lenguist.github.io/site/from_scratch/2021/03/12/nn-from-scratch.html",
            "relUrl": "/from_scratch/2021/03/12/nn-from-scratch.html",
            "date": " • Mar 12, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://lenguist.github.io/site/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://lenguist.github.io/site/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://lenguist.github.io/site/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}