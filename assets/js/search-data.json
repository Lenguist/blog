{
  
    
        "post0": {
            "title": "What is AI __really__?",
            "content": "What is AI really? . Intro . So I was doing computer vision tutorial on kaggle and it had been really interesting to learn imaage processing AFTER NLP and not vice versa. It’s remarkable hwo similar the development of those fields are, and I had some interesting thoughts on it. Enjoy! . AI vs DL vs ML vs Data Science . From my experience/knowledge/uderstanding, AI is a very specific term which is used somewhat interchangeable with ML and DL by many people. It is interesting how when learning more stuff about ML/DL/AI this distinction became more clear and intuitive for me. Data science is the field of data analysis in general, so pretty much anything you do involving data is data science. The term is so broad it is somewhat useless in my opinion. Then we have ML. ML is about statistics and “learning” data representations. I do not remember where I heard it, but someone said ML is statictics plus, and that is kind of how I like to think about it. It is a very math-heavy field, and, imo, pretty boring one. Saying that, I also did not really do much ML, so that is a pretty uneducated opinion. . Then we have DL. Deep learning, by definition, is just a type of machine learning which uses “deep nets.” And I thinnk that is … a good definition. It leads, ultimately, to the less obvious differences between ML annd DL. Because we have many hidden layers, DL necesarily acts like a black box. Also, DL is much less statistics and much more architecture and feature engineereing. Here you have CNNs, and RNNs, and Transformers, and GANs, and those “architectures” are much more conceptually based then ML algorithms. Like, youu get an idea for what RNN is and why you need and when you need because the structure itself is a result of human intuition rather than careful proof. There is nno proof that RNNs would be good for sequential data; there is probably not even a way to rigurously define “good” in this context. It is not like some mathematician somewhere proved that yeah, if you have sequential data, rnns would be great. It’s an intuitive idea, which turned out to be true, and that’s really fascinating. In DL,architectures are modeled by the way we, humans, think about stuff. Embeddings are not something “the statistics” would suggest as a solution; it is purely taken from the way humans think about words and complex categorization. For me, this what makes DL a much more compelling field then ML: the fact that it is “closer” to the mind, and closer to AI. . And what is AI? Is image recognition an AI? Is voice detection an AI? Is alpha Zero a AI? Is Dall-e an AI? Is GPT an AI? For me, some of those examples feel more like AI then others, but it is hard to pinpoint what makes them “more AI.” I think it might be the degree to which they can innterract with humans? That is a bad definion though; it has nothing to do with the AI itself, and more with how we use it. Still though, image recognition system is predictable: it will recognize images. We kind of, in a way, have a clea expectation of what it is supposed to do. Same with voice detection. Alpha zero is … trickier. It’s suposed to be good at chess, but we dont have such specific expectations for its actions. We dont wait for it to make a particular move. In a way, it feels “more-AI” because it surpasses humans in its activity, making us unable to set concrete expectations. I think the same goes for Dall-e and GPT: they are so good, we do not know what to expect to of them. But there is more to it: they are “generative.” I put quotation marks around that beccause in DL the term generative model has a specific meaning and technically all five would be “generating” something. By generating I more likely mean “repurposing” human input. Image recognition does not create new information for us, humans. Same with voie detection. Alpha Zero makes a move based on human input, not only “translating it.” Same with Dall-e or GPT - they generate new information on human input, and not just convert the information from one form to another. . I got off track. So, for me, AI seems like too broad of a definiton really. I think AI must be neccesarily an agent which a human has to be able to interact with. Image recognition is not AI in my opinion: it is a DL model with a very specific, predefined purpose. What makes AI “more-AI” is it’s capability to create unique information. . Wow. That was a nice revelation. So yeah, I also want to talk about use cases of AI vs DL, but for now, let’s talk more about this “more-AI” concept because it is really fascinating. What makes AGI? ability to create unique information without human input. In other words, it has to be self-sufficient. I mean, this is so huge. It has to be adaptive, and it has to self-sufficient. . So … let’s talk about those conditions. . Adaptive AI . I think this is gonna be a major research topic in the coming years. Because now, with the advent of “lucky ticket” concept for neural nets, the magic kinda wastes away and you see them for what they are - limited, rigid, functions, which we have a convoluted way to findd and define. But, inteligence is more than that. Ultimately, it’s not even the fact that human intelligence is different from DL models, it’s the fact those models fail in important ways because they lack adaptability. Again, as I am thinking about a possible solution here, I am coming back to human brainn and how it works, which is kinda interesting. You dont ddo that much in ML; there, you turn to math. I mean maybe you should turn to math here as well. What is a way to make something adaptable? What do we mean by adaptable? How much data should the model need to adapt? We dont want it freaking out with every example, but we also dont want to wait years to have enough data for a retrain. And also the process of training should be .. constant? how woudl that work? Maybe doing some reflexive module, where the model trains trying to understand itself? Or weight new examples higher than old to make them more relevant? Or just collect mroe data, so that the continuous training concept makes sense. Our minds are laways in motion; we never stop thinking. The model does; it never thinks. It’s a function, a state of AI, so to say. what we call AI is really a snapshot of an AI. . Which brings us to reinforcement learning? to make AI adaptive, we want it to learn from experiences. That sentence made 0 sense. I feel like RL is the solution, my intuition tells me that, but I can not quite put into words why I think that. . Perceptive AI . I will add this category in, because that’s what I wanted to talk about initially, but I will go over it briefly. Humans collect a lot of data to worm with constantly. We need AI doing the same. Text + image + voice, but all in one model. . To be continued… . My brain hurts. I need to turn this into a series, add pictures, and be mroe concise with my writing. Will do tmrw. .",
            "url": "https://lenguist.github.io/site/thoughts/2021/03/13/perceptive-ai.html",
            "relUrl": "/thoughts/2021/03/13/perceptive-ai.html",
            "date": " • Mar 13, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Seaborn",
            "content": ". Hello, Seaborn . import pandas as pd pd.plotting.register_matplotlib_converters() import matplotlib.pyplot as plt import seaborn as sns . fifa_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/fifa.csv&quot; # Read the file into a variable fifa_data fifa_data = pd.read_csv(fifa_filepath, index_col=&quot;Date&quot;, parse_dates=True) # Set the width and height of the figure plt.figure(figsize=(16,6)) # Line chart showing how FIFA rankings evolved over time sns.lineplot(data=fifa_data) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa69ac0e0d0&gt; . Line Charts . spotify_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/spotify.csv&quot; # Read the file into a variable spotify_data spotify_data = pd.read_csv(spotify_filepath, index_col=&quot;Date&quot;, parse_dates=True) # Set the width and height of the figure plt.figure(figsize=(14,6)) # Add title plt.title(&quot;Daily Global Streams of Popular Songs in 2017-2018&quot;) sns.lineplot(data=spotify_data) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa69ac6fdd0&gt; . plt.figure(figsize=(14,6)) # Add title plt.title(&quot;Daily Global Streams of Popular Songs in 2017-2018&quot;) # Line chart showing daily global streams of &#39;Shape of You&#39; sns.lineplot(data=spotify_data[&#39;Shape of You&#39;], label=&quot;Shape of You&quot;) # Line chart showing daily global streams of &#39;Despacito&#39; sns.lineplot(data=spotify_data[&#39;Despacito&#39;], label=&quot;Despacito&quot;) # Add label for horizontal axis plt.xlabel(&quot;Date&quot;) . Text(0.5, 0, &#39;Date&#39;) . Bar Charts and Heatmaps . flight_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/flight_delays.csv&quot; # Read the file into a variable flight_data flight_data = pd.read_csv(flight_filepath, index_col=&quot;Month&quot;) . plt.figure(figsize=(10,6)) # Add title plt.title(&quot;Average Arrival Delay for Spirit Airlines Flights, by Month&quot;) # Bar chart showing average arrival delay for Spirit Airlines flights by month sns.barplot(x=flight_data.index, y=flight_data[&#39;NK&#39;]) # Add label for vertical axis plt.ylabel(&quot;Arrival delay (in minutes)&quot;) . Text(0, 0.5, &#39;Arrival delay (in minutes)&#39;) . plt.figure(figsize=(14,7)) # Add title plt.title(&quot;Average Arrival Delay for Each Airline, by Month&quot;) # Heatmap showing average arrival delay for each airline by month sns.heatmap(data=flight_data, annot=True) # Add label for horizontal axis plt.xlabel(&quot;Airline&quot;) . Text(0.5, 42.0, &#39;Airline&#39;) . Scatter Plots . insurance_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/insurance.csv&quot; # Read the file into a variable insurance_data insurance_data = pd.read_csv(insurance_filepath) . sns.scatterplot(x=insurance_data[&#39;bmi&#39;], y=insurance_data[&#39;charges&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa69af65c50&gt; . sns.regplot(x=insurance_data[&#39;bmi&#39;], y=insurance_data[&#39;charges&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa691cc0f90&gt; . sns.scatterplot(x=insurance_data[&#39;bmi&#39;], y=insurance_data[&#39;charges&#39;], hue=insurance_data[&#39;smoker&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa691b73650&gt; . sns.lmplot(x=&quot;bmi&quot;, y=&quot;charges&quot;, hue=&quot;smoker&quot;, data=insurance_data) . &lt;seaborn.axisgrid.FacetGrid at 0x7fa68d375d10&gt; . sns.swarmplot(x=insurance_data[&#39;smoker&#39;], y=insurance_data[&#39;charges&#39;]) . /usr/local/lib/python3.7/dist-packages/seaborn/categorical.py:1296: UserWarning: 67.3% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa68d286c10&gt; . Distributions . iris_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/iris.csv&quot; # Read the file into a variable iris_data iris_data = pd.read_csv(iris_filepath, index_col=&quot;Id&quot;) # Print the first 5 rows of the data iris_data.head() . Sepal Length (cm) Sepal Width (cm) Petal Length (cm) Petal Width (cm) Species . Id . 1 5.1 | 3.5 | 1.4 | 0.2 | Iris-setosa | . 2 4.9 | 3.0 | 1.4 | 0.2 | Iris-setosa | . 3 4.7 | 3.2 | 1.3 | 0.2 | Iris-setosa | . 4 4.6 | 3.1 | 1.5 | 0.2 | Iris-setosa | . 5 5.0 | 3.6 | 1.4 | 0.2 | Iris-setosa | . sns.distplot(a=iris_data[&#39;Petal Length (cm)&#39;], kde=False) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa68d1b5990&gt; . sns.kdeplot(data=iris_data[&#39;Petal Length (cm)&#39;], shade=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa68d12ec10&gt; . sns.jointplot(x=iris_data[&#39;Petal Length (cm)&#39;], y=iris_data[&#39;Sepal Width (cm)&#39;], kind=&quot;kde&quot;, shade=True) . &lt;seaborn.axisgrid.JointGrid at 0x7fa68cd39f50&gt; . iris_set_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/iris_setosa.csv&quot; iris_ver_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/iris_versicolor.csv&quot; iris_vir_filepath = &quot;/content/drive/MyDrive/Code/kaggle/data/iris_virginica.csv&quot; # Read the files into variables iris_set_data = pd.read_csv(iris_set_filepath, index_col=&quot;Id&quot;) iris_ver_data = pd.read_csv(iris_ver_filepath, index_col=&quot;Id&quot;) iris_vir_data = pd.read_csv(iris_vir_filepath, index_col=&quot;Id&quot;) . sns.distplot(a=iris_set_data[&#39;Petal Length (cm)&#39;], label=&quot;Iris-setosa&quot;, kde=False) sns.distplot(a=iris_ver_data[&#39;Petal Length (cm)&#39;], label=&quot;Iris-versicolor&quot;, kde=False) sns.distplot(a=iris_vir_data[&#39;Petal Length (cm)&#39;], label=&quot;Iris-virginica&quot;, kde=False) # Add title plt.title(&quot;Histogram of Petal Lengths, by Species&quot;) # Force legend to appear plt.legend() . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.legend.Legend at 0x7fa68d0e7150&gt; . sns.kdeplot(data=iris_set_data[&#39;Petal Length (cm)&#39;], label=&quot;Iris-setosa&quot;, shade=True) sns.kdeplot(data=iris_ver_data[&#39;Petal Length (cm)&#39;], label=&quot;Iris-versicolor&quot;, shade=True) sns.kdeplot(data=iris_vir_data[&#39;Petal Length (cm)&#39;], label=&quot;Iris-virginica&quot;, shade=True) # Add title plt.title(&quot;Distribution of Petal Lengths, by Species&quot;) . Text(0.5, 1.0, &#39;Distribution of Petal Lengths, by Species&#39;) . Choosing Plot Types and Custom Styles . plt.figure(figsize=(12,6)) sns.lineplot(data=spotify_data) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa68d55da90&gt; . sns.set_style(&quot;dark&quot;) # Line chart plt.figure(figsize=(12,6)) sns.lineplot(data=spotify_data) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa68cad4750&gt; .",
            "url": "https://lenguist.github.io/site/kaggle/2021/03/12/seaborn.html",
            "relUrl": "/kaggle/2021/03/12/seaborn.html",
            "date": " • Mar 12, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Neural network from scratch",
            "content": ". I am currently on a plane from Frankfurt to Newark, and I can&#39;t fall asleep (probably because of the 2 cups of cofee I drank). So here is a challenge: I need to write a neural network from scratch, using only basic libraries (pandas, numpy, matplotlib). No internet, no tutorials, only me, libs documentation, an abundance of free time and existential dread of being in a metal box 12 km above the Atlantic. Let&#39;s fucking go. . Okay so first thigns first what the hell am I trying to do here. I initially tried to do smth based on the datasets I have downloaded, but all of them are fro text analysis and I soundly decided that coding embeddings from scratch would not be particularly fun (though maybe one day...). NB: Apparently I did have some downloaded from the kaggle visualization course, but when I found that out I pivoted anyway so duh. NB2: Actually coding rnn from scratch sounds really fun. Definitely should do it some day. . So instead I will be generating some synthetic data. Initially I wanted to do classification cause regressions is boring, but after delving deeper into the task I understood I grossly underestimated how hard it is to do ANYTHING with those basic libs, so I won&#39;t be too picky. First I wanted to fit a quadratic, but then when I understood I will have to do backprop by hand I decided to hell with it I will do the bare minimum which works. So yeah. I will generate some noise linear dataset, do a very small classic network (perceptron), and try to make it work. Somehow. . Setup . import numpy as np import matplotlib as plt import pandas as pd . Generating data for the regression . Okay this was actually harder than I though cause random functions in numpy dont work the way you woudl expect. Apparently you can&#39;t generate random reals only in range (0,1), so I did some quick fixes to circumvent that. But overall, it actually works! Unbelievable . data = [] for i in range(1000): x = np.random.rand()*20 noise = np.random.randn() y = 6.9*(x+noise) + 6.9*noise #nice. data.append([x,y]) import matplotlib as plt regression_data = pd.DataFrame(data) regression_data.plot(x=0, y=1, kind=&#39;scatter&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc4a768b910&gt; . Helper functions . Apparently numpy is even more limited than I thought. Or maybe I am a dumbass and can&#39;t find the right funcs. Whatever. Defined some helpers functions used later on . def sigmoid(x): return 1/(1+np.exp(-x)) def d_sigmoid(x): return np.exp(-x)/((1+np.exp(-x))**2) def random_sign(x): if np.random.rand()&gt;0.5: det = 1 else: det = -1 return x*det def initialize(): return random_sign(np.random.randint(10)/10) . Model . Okaaaay. I scratched my head for a while here. I totally forgot how exactly back prop works. Also apprently I forgot calculust and linear algebra. Took me like half an hour to recall how derivatives work and how matrix multiplication works. But in the end, I got the right idea. The problem is, the elegant and simple layer-as-a-matrix-multiplication approach does not work cause numpy has no gradient, nothing. And defining gradient from scratch means I need a way to access individual derivatives ... . NB: actually there should have been a way for me to do this elegantly either way. As it turned out later operations performed for each weight and each layer are really similar and could have been written as applying some transformation to an array. I should revise it some day, probably. . So anyway. After some thinking I decided to abandon elegance and go for something which, well, I can make work. So instead of treating weights as matrices, and neurons and biases as vectors, I treat everything as a number, plain and a simple. The problem is ... that&#39;s a lot of numbers. Even for a modest 1-8-8-8-1 network that&#39;s almost 200 weights and 25 biases. So I scaled my ambitions back a bit to a 1-2-3-1 network. I am ot even sure it would be enough to fit a quadratic, but my data is linear so it should in theory at least handle it. I ended up with 11 weights and 6 biases. I will be using logistic sigmoid for my activation layer, and I will apply for the hidden layers only. . ################################################################################# #Notation #for weights: #first number - layer, second number - input neuron, third number - output neuron #for biases: #first number-layer, second number - neuron ################################################################################# class myNetwork(object): def __init__(self): #Initializing weights #First layer self.w111=initialize() self.w112=initialize() self.b11=initialize() self.b12=initialize() #Second layer self.w211=initialize() self.w221=initialize() self.w212=initialize() self.w222=initialize() self.w213=initialize() self.w223=initialize() self.b21=initialize() self.b22=initialize() self.b23=initialize() #Third layer self.w311=initialize() self.w321=initialize() self.w331=initialize() self.b31=initialize() #accepts a list in the form of [x,y] def forward(self, datapoint, predict=False): self.n0 = datapoint[0] #First layer self.n11 = sigmoid(self.w111*self.n0+self.b11) self.n12 = sigmoid(self.w112*self.n0+self.b12) #Second layer self.n21 = sigmoid(self.w211*self.n11+self.w221*self.n12+self.b21) self.n22 = sigmoid(self.w212*self.n11+self.w222*self.n12+self.b22) self.n23 = sigmoid(self.w213*self.n11+self.w223*self.n12+self.b23) #Third layer self.n31 = self.w311*self.n21+self.w321*self.n22+self.w331*self.n23+self.b31 out = self.n31 if predict: return out else: #Computing loss #loss function loss = (out-datapoint[1])**2 return out, loss def backpropagate(self,datapoint, out, loss): #Backpropagation #d_# denotes a derivative of loss with respect to # d_out = 2*out - 2*datapoint[1] #out = w311*n21+w321*n22+w331*n23+b31 self.d_w311 = d_out*self.n21 self.d_w321 = d_out*self.n22 self.d_w331 = d_out*self.n23 self.d_b31 = d_out d_n21 = d_out*self.w311 d_n22 = d_out*self.w321 d_n23 = d_out*self.w331 #n21 = sigmoid(w211*n11+w221*n12+b21) d_inner_n21 = d_n21*d_sigmoid(self.w211*self.n11+self.w221*self.n12+self.b21) self.d_w211 = d_inner_n21*self.n11 self.d_w221 = d_inner_n21*self.n12 self.d_b21 = d_inner_n21 d_n21_n11 = d_inner_n21*self.d_w211 d_n21_n12 = d_inner_n21*self.d_w221 #n22 = sigmoid(w212*n11+w222*n12+b22) d_inner_n22 = d_n22*d_sigmoid(self.w212*self.n11+self.w222*self.n12+self.b22) self.d_w212 = d_inner_n22*self.n11 self.d_w222 = d_inner_n22*self.n12 self.d_b22 = d_inner_n22 d_n22_n11 = d_inner_n22*self.d_w212 d_n22_n12 = d_inner_n22*self.d_w222 #n23 = sigmoid(w213*n11+w223*n12+b23) d_inner_n23 = d_n23*d_sigmoid(self.w213*self.n11+self.w223*self.n12+self.b23) self.d_w213 = d_inner_n23*self.n11 self.d_w223 = d_inner_n23*self.n12 self.d_b23 = d_inner_n23 d_n23_n11 = d_inner_n21*self.d_w213 d_n23_n12 = d_inner_n21*self.d_w223 ############################## d_n11 = d_n21_n11+d_n22_n11+d_n23_n11 d_n12 = d_n21_n12+d_n22_n12+d_n23_n12 #n11 = sigmoid(w111*n0+b11) d_inner_n11 = d_n11*d_sigmoid(self.w111*self.n0+self.b11) self.d_w111 = d_inner_n11*self.n0 self.d_b11 = d_inner_n11 #n12 = sigmoid(w112*n0+b12) d_inner_n12 = d_n12*d_sigmoid(self.w112*self.n0+self.b12) self.d_w112 = d_inner_n12*self.n0 self.d_b12 = d_inner_n12 def update_weights(self,lr): #I will be using standard SGD, so no fancy optimizers and schedulers. #Update params #First layer self.w111 -= self.d_w111*lr self.w112 -= self.d_w112*lr self.b11 -= self.d_b11*lr self.b12 -= self.d_b12*lr #Second layer self.w211 -= self.d_w211*lr self.w221 -= self.d_w221*lr self.w212 -= self.d_w212*lr self.w222 -= self.d_w222*lr self.w213 -= self.d_w213*lr self.w223 -= self.d_w223*lr self.b21 -= self.d_b21*lr self.b22 -= self.d_b22*lr self.b23 -= self.d_b23*lr #Third layer self.w311 -= self.d_w311*lr self.w321 -= self.d_w321*lr self.w331 -= self.d_w331*lr def train(self, dataset, n_epochs, lr): losses = [] for i in range(n_epochs): epoch_loss = 0 i += 1 #print(&quot;=&quot;*20) #print(&quot;Performing epoch &quot; + str(i)) for datapoint in dataset: out, loss = self.forward(datapoint) epoch_loss+=loss self.backpropagate(datapoint,out,loss) self.update_weights(lr) losses.append(epoch_loss) #Omitting first epoch loss cause that epoch is just shit return pd.DataFrame(losses[1:]).plot() def predict(self, x): out = self.forward([x], predict=True) return out . Is this good code? No. Does it work? No actually:) I think my network might be too small for the task. whatever) It compiles))) Here are the results: . net = myNetwork() . net.train(dataset=data, n_epochs=47, lr=0.01) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc4a7783610&gt; . As you can see the loss goes down the way you would expect it to. Which is nice. But also it&#39;s kinda high in the end ... Here is why ... . Input data . regression_data = pd.DataFrame(data) regression_data.plot(x=0, y=1, kind=&#39;scatter&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc4a7886850&gt; . Output data . points = [] for i in range(0,20): points.append(net.predict(i)) points = pd.DataFrame(points) points.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc4a7aaa750&gt; . I mean... Theya are not even close))))))) But it does train and it is kinda close -- the problem is not in the model itself, but more in the model-dataset fit. Also, I did not use any regularization, so that may affect stuff. Regardless ... It was a fun experience! Hopefully I can come back to this one day and fix it for good))) .",
            "url": "https://lenguist.github.io/site/from_scratch/2021/03/12/nn-from-scratch.html",
            "relUrl": "/from_scratch/2021/03/12/nn-from-scratch.html",
            "date": " • Mar 12, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://lenguist.github.io/site/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "AI researcher and developer from Ukraine. Currently studying at Lawrenceville school, NJ, USA. .",
          "url": "https://lenguist.github.io/site/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://lenguist.github.io/site/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}